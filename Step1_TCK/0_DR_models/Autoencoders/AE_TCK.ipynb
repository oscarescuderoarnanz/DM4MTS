{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import AE_models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras \n",
    "from keras import layers\n",
    "import scipy.io as sio\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_range_analysis(X):\n",
    "    if sum(sum(((np.round(X,4) <= 1)) & (X >= 0))) == X.shape[0] * X.shape[1]:\n",
    "        print(\"Range value: [0,1]\")\n",
    "    else:\n",
    "        print(\"ERROR!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLayers(plot_first_layer, plot_second_layer, dicc, title, carpeta):\n",
    "    plot_first_layer = plot_first_layer\n",
    "    plot_second_layer = plot_second_layer\n",
    "\n",
    "    scores = dicc[0]\n",
    "    scoresTrain = dicc[1]\n",
    "\n",
    "    fig = plt.figure(figsize=[20, 20])\n",
    "    ax = fig.add_subplot(1,2,1, projection='3d')\n",
    "    XX, YY = np.meshgrid(plot_first_layer, plot_second_layer)\n",
    "\n",
    "    cluster = ['r','b','g','k','m', 'c','y', 'r', 'b']\n",
    "    marker = ['.', 'o', '^', 'p', '*', 'v', 's', '.', 'o'] \n",
    "\n",
    "    for xp, yp, sco, c, m in zip(XX, YY, scores, cluster, marker):\n",
    "        surf = ax.scatter([xp], [yp], [sco], cmap=cm.coolwarm,\n",
    "                               linewidth=3, antialiased=False, color = c, marker = '.')\n",
    "\n",
    "    ax.set_xlabel('plot_first_layer'); ax.set_ylabel('plot_second_layer'); ax.set_zlabel('val loss')\n",
    "\n",
    "    ax.set_zlim([np.min(scores), np.max(scores)+0.1]);\n",
    "    ax.set_xticks(plot_first_layer); \n",
    "    ax.set_yticks(plot_second_layer);\n",
    "    ax.view_init(elev=10, azim=12);\n",
    "    ax.set_title('Val loss - ' + str(title))\n",
    "\n",
    "    ax = fig.add_subplot(1,2,2, projection='3d')\n",
    "    XX, YY = np.meshgrid(plot_first_layer, plot_second_layer)\n",
    "\n",
    "    \n",
    "    for xp, yp, sco, c, m in zip(XX, YY, scoresTrain, cluster, marker):\n",
    "        surf = ax.scatter([xp], [yp], [sco], cmap=cm.coolwarm,\n",
    "                               linewidth=3, antialiased=False, color = c, marker = 'X')\n",
    "\n",
    "    ax.set_xlabel('plot_first_layer'); ax.set_ylabel('plot_second_layer'); ax.set_zlabel('loss')\n",
    "\n",
    "    ax.set_zlim([np.min(scores), np.max(scores)+0.1]);\n",
    "    ax.set_xticks(plot_first_layer); \n",
    "    ax.set_yticks(plot_second_layer);\n",
    "    ax.view_init(elev=10, azim=12);\n",
    "    ax.set_title('Loss - ' + str(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLayers_vf(h_layers, dicc, carpeta):\n",
    "    plot_first_layer = []\n",
    "    plot_second_layer = []\n",
    "    for i in range(len(h_layers)):\n",
    "        plot_first_layer.append(h_layers[i][1])\n",
    "        plot_second_layer.append(h_layers[i][2])\n",
    "\n",
    "    keys = list(dicc.keys())\n",
    "    for i in range(len(keys)):\n",
    "        plotLayers(plot_first_layer, plot_second_layer, dicc[keys[i]], keys[i], carpeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "normalizar = True\n",
    "plotAndSaveImg = False\n",
    "\n",
    "for index_data in range(len(folders)):\n",
    "    print(\"================================================================>\" + folders[index_data] + \"<=======================================================\")\n",
    "    X_train_original, X_test_original, y_train_original, y_test_original = AE_models.loadData(index_data, normalizar, folders)\n",
    "\n",
    "    autoencodertype = {'DAE': False, 'AE': True}\n",
    "\n",
    "\n",
    "    epochs = 5000\n",
    "    initial_learning_rate = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    batch_size = [32]\n",
    "    dropout=[0, .05, .1]\n",
    "\n",
    "    h_layers = [[X_train_original.shape[0], 600],\n",
    "                [X_train_original.shape[0], 575],\n",
    "                [X_train_original.shape[0], 550],\n",
    "                [X_train_original.shape[0], 525],\n",
    "                [X_train_original.shape[0], 500],\n",
    "                [X_train_original.shape[0], 470],\n",
    "                [X_train_original.shape[0], 450],\n",
    "                [X_train_original.shape[0], 425],\n",
    "                [X_train_original.shape[0], 400],\n",
    "                [X_train_original.shape[0], 375],\n",
    "                [X_train_original.shape[0],  350],\n",
    "                [X_train_original.shape[0],  325]]\n",
    "    \n",
    "    hyperparameters = {'epochs': epochs,\n",
    "                       'initial_learning_rate':initial_learning_rate,\n",
    "                       'kfold':5,\n",
    "                       \"mindelta\": 0.0001}\n",
    "\n",
    "    y_pre_train = y_train_original\n",
    "    y_test = y_test_original\n",
    "    X_pre_train = X_train_original\n",
    "    X_test = X_test_original\n",
    "\n",
    "    semillas = [9,18,35, 52, 75]\n",
    "\n",
    "    kf = KFold(n_splits=hyperparameters[\"kfold\"], shuffle=True, random_state=semillas[index_data])\n",
    "    kf.get_n_splits(X_pre_train)\n",
    "\n",
    "    bestHyperparameters = {'h_layers': -1, 'batch_size': -1, 'initial_learning_rate': -1 }\n",
    "    dicc = {}\n",
    "    bestMetricDev = np.inf\n",
    "    for d in range(len(dropout)):\n",
    "        for init in range(len(initial_learning_rate)):\n",
    "            for bs in range(len(batch_size)):\n",
    "                plot_val_loss_mean = []\n",
    "                plot_loss_mean = []\n",
    "                for l in range(len(h_layers)):\n",
    "\n",
    "                    hyperparameters_aux = {'epochs': hyperparameters['epochs'],\n",
    "                                       'initial_learning_rate': hyperparameters['initial_learning_rate'][init],\n",
    "                                       \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "                                       'batch_size': batch_size[bs],\n",
    "                                       'h_layers': h_layers[l],\n",
    "                                       'dropout': dropout[d],\n",
    "                                       'verbose': 0}\n",
    "\n",
    "                    v_metric_dev = []\n",
    "                    val_loss = []\n",
    "                    loss = []\n",
    "                    for train_index, val_index in kf.split(X_pre_train):\n",
    "                        X_train, X_val = X_pre_train[train_index], X_pre_train[val_index]\n",
    "                        # Reset keras\n",
    "                        AE_models.reset_keras()\n",
    "                        # Run the model\n",
    "                        model, hist, early = AE_models.runNetwork(X_train, X_val,\n",
    "                                                                  hyperparameters_aux,\n",
    "                                                                  autoencodertype)\n",
    "\n",
    "\n",
    "                        loss.append(np.min(hist.history[\"loss\"]))\n",
    "                        val_loss.append(np.min(hist.history[\"val_loss\"]))\n",
    "\n",
    "                    plot_val_loss_mean.append(np.mean(val_loss))\n",
    "                    plot_loss_mean.append(np.mean(loss))   \n",
    "                    metric_dev = np.mean(val_loss)\n",
    "                    if metric_dev < bestMetricDev:\n",
    "                        print(\"\\tChange the best \", bestMetricDev, \" by metric dev: \", metric_dev)\n",
    "                        bestMetricDev = metric_dev\n",
    "                        bestHyperparameters['h_layers'] = l\n",
    "                        bestHyperparameters['batch_size'] = bs\n",
    "                        bestHyperparameters['initial_learning_rate'] = init\n",
    "                        bestHyperparameters['dropout'] = d\n",
    "                        print(\"%%%%%%%%%bestHyperparameters%%%%%%%%%\")\n",
    "                        print(bestHyperparameters)\n",
    "                \n",
    "                dicc[\"bs:\" + str(batch_size[bs]) + \"-lr:\" + str(initial_learning_rate[init]) + \"-drop:\" + str(dropout[d])] = [plot_val_loss_mean, plot_loss_mean]\n",
    "\n",
    "\n",
    "    if plotAndSaveImg:\n",
    "        plotLayers_vf(h_layers, dicc, folders[index_data])\n",
    "\n",
    "    print(\"Layers selected:\", h_layers[bestHyperparameters[\"h_layers\"]])\n",
    "    print(\"batch_size selected:\", batch_size[bestHyperparameters[\"batch_size\"]])\n",
    "    print(\"initial_learning_rate selected:\", initial_learning_rate[bestHyperparameters[\"initial_learning_rate\"]])\n",
    "    print(\"dropout selected:\", dropout[bestHyperparameters[\"dropout\"]])\n",
    "\n",
    "    print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%================================================================> BEST MODEL SELECT WITH DIFFERENTS HYPERPARAMETERS <=======================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "    hyperparameters = {'epochs': hyperparameters['epochs'],\n",
    "                   'initial_learning_rate': initial_learning_rate[bestHyperparameters[\"initial_learning_rate\"]],\n",
    "                   \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "                   'batch_size': batch_size[bestHyperparameters[\"batch_size\"]],\n",
    "                   'h_layers': h_layers[bestHyperparameters[\"h_layers\"]],\n",
    "                   'dropout': dropout[bestHyperparameters[\"dropout\"]],\n",
    "                   'verbose': 1}\n",
    "    \n",
    "    AE_models.reset_keras()\n",
    "    X_train_aux, X_val_aux = train_test_split(X_pre_train,\n",
    "                                              test_size=0.3,\n",
    "                                              random_state=semillas[index_data])\n",
    "    \n",
    "    autoencoder, hist, early = AE_models.runNetwork(X_train_aux, X_val_aux,\n",
    "                                                    hyperparameters,\n",
    "                                                    autoencodertype)\n",
    "\n",
    "    encoder = keras.Model(autoencoder.input, autoencoder.get_layer('Latent_layer').output)\n",
    "    #Load the data\n",
    "    X_train = sio.loadmat('../../data_reduced_by_tck/' + folders[index_data] + '/Ktrtr')\n",
    "    X_test = sio.loadmat('../../data_reduced_by_tck/' + folders[index_data] + '/Ktrte')\n",
    "    X_train = X_train['Ktrtr']\n",
    "    X_test = X_test['Ktrte']\n",
    "    X_test = X_test.T\n",
    "\n",
    "    X_train, X_test = AE_models.normData_minmax(X_train, X_test)\n",
    "\n",
    "    encoder = keras.Model(autoencoder.input, autoencoder.get_layer('Latent_layer').output)\n",
    "    # Encode the data\n",
    "    X_train_encode = encoder.predict(X_train)\n",
    "    X_test_encode = encoder.predict(X_test)\n",
    "\n",
    "    pd.DataFrame(X_train_encode).to_csv(\"../../1_Clasifications_models/data_reduced/AE/X_train_Norm_AE_\" + folders[index_data] + \".csv\", index=False)\n",
    "    pd.DataFrame(X_test_encode).to_csv(\"../../1_Clasifications_models/data_reduced/AE/X_test_Norm_AE_\" + folders[index_data] + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
