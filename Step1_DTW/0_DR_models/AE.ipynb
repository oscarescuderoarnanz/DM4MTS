{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import AE_models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras \n",
    "from keras import layers\n",
    "import scipy.io as sio\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_range_analysis(X):\n",
    "    if sum(sum(((np.round(X,4) <= 1)) & (X >= 0))) == X.shape[0] * X.shape[1]:\n",
    "        print(\"Range value: [0,1]\")\n",
    "    else:\n",
    "        print(\"ERROR!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLossesLayers(h_layers, dicc, carpeta, dependencia):\n",
    "    plot_layers = []\n",
    "    for i in range(len(h_layers)):\n",
    "        plot_layers.append(h_layers[i][1])\n",
    "\n",
    "    keys = list(dicc.keys())\n",
    "    \n",
    "    print(len(keys))\n",
    "    filas = int(np.sqrt(np.ceil(len(keys)/2)))\n",
    "    print(filas)\n",
    "    columnas = int(np.sqrt(np.round(len(keys)/2)))\n",
    "    print(columnas)\n",
    "    \n",
    "    f, axis = plt.subplots(filas, columnas, figsize=(20,20))\n",
    "    index_keys = 0\n",
    "    for i in range(filas):\n",
    "        for j in range(columnas):\n",
    "            # error in validation\n",
    "            axis[i,j].plot(plot_layers, dicc[keys[index_keys]][0])\n",
    "            # error in train\n",
    "            axis[i,j].plot(plot_layers, dicc[keys[index_keys]][1])\n",
    "            axis[i,j].legend([\"val_loss\", \"loss\"])\n",
    "            axis[i,j].set_title(keys[i])\n",
    "            index_keys += 1\n",
    "\n",
    "    plt.close(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folders = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "normalizar = True\n",
    "plotAndSaveImg = False\n",
    "dependencia = \"DTW_D\"\n",
    "\n",
    "for index_data in range(len(folders)):\n",
    "\n",
    "    print(\"================================================================>\" + folders[index_data] + \"<=======================================================\")\n",
    "    X_train_original, X_test_original, y_train_original, y_test_original = AE_models.loadData(index_data, normalizar, folders, dependencia)\n",
    "\n",
    "    # TYPE OF AUTOENCODER\n",
    "    autoencodertype = {'DAE': False, 'AE': True}\n",
    "\n",
    "\n",
    "    epochs = 5000\n",
    "    initial_learning_rate = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    batch_size = [32]\n",
    "    dropout=[0, .05, .1]\n",
    "\n",
    "    h_layers_1 = [[X_train_original.shape[0], 450],\n",
    "                [X_train_original.shape[0], 425],\n",
    "                [X_train_original.shape[0], 400],\n",
    "                [X_train_original.shape[0], 375],\n",
    "                [X_train_original.shape[0],  350],\n",
    "                [X_train_original.shape[0],  325],\n",
    "                [X_train_original.shape[0],  300],\n",
    "                [X_train_original.shape[0], 275],\n",
    "                [X_train_original.shape[0], 250]]\n",
    "    \n",
    "    hyperparameters = {'epochs': epochs,\n",
    "                       'initial_learning_rate':initial_learning_rate,\n",
    "                       'kfold':5,\n",
    "                       \"mindelta\": 0.0001}\n",
    "\n",
    "    y_pre_train = y_train_original\n",
    "    y_test = y_test_original\n",
    "    X_pre_train = X_train_original\n",
    "    X_test = X_test_original\n",
    "    \n",
    "    gamma = 0.8\n",
    "    X_pre_train = np.exp(-gamma*X_pre_train)\n",
    "    X_test = np.exp(-gamma*X_test)\n",
    "\n",
    "    semillas = [9,18,35, 52, 75]\n",
    "\n",
    "    kf = KFold(n_splits=hyperparameters[\"kfold\"], shuffle=True, random_state=semillas[index_data])\n",
    "    kf.get_n_splits(X_pre_train)\n",
    "\n",
    "    bestHyperparameters = {'h_layers': -1, 'batch_size': -1, 'initial_learning_rate': -1 }\n",
    "\n",
    "    bestMetricDev = np.inf\n",
    "    dicc_1 = {}\n",
    "    for d in range(len(dropout)):\n",
    "        for init in range(len(initial_learning_rate)):\n",
    "            for bs in range(len(batch_size)):\n",
    "                plot_val_loss = []\n",
    "                plot_loss = []\n",
    "                for l in range(len(h_layers_1)):\n",
    "                    hyperparameters_aux = {'epochs': hyperparameters['epochs'],\n",
    "                                       'initial_learning_rate': hyperparameters['initial_learning_rate'][init],\n",
    "                                       \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "                                       'batch_size': batch_size[bs],\n",
    "                                       'h_layers': h_layers_1[l],\n",
    "                                       'dropout': dropout[d],\n",
    "                                       'verbose': 0}\n",
    "\n",
    "                    v_early = []\n",
    "                    v_metric_dev = []\n",
    "                    v_hist = []\n",
    "                    v_val_loss = []\n",
    "                    for train_index, val_index in kf.split(X_pre_train):\n",
    "                        \n",
    "                        X_train, X_val = X_pre_train[train_index], X_pre_train[val_index]\n",
    "                        # Reset keras\n",
    "                        AE_models.reset_keras()\n",
    "                        # Train the network and test it \n",
    "                        model, hist, early = AE_models.runNetwork(X_train, X_val,\n",
    "                                                                  hyperparameters_aux,\n",
    "                                                                  autoencodertype)\n",
    "\n",
    "                        v_early.append(early)\n",
    "                        v_hist.append(hist)\n",
    "                        v_val_loss.append(np.min(hist.history[\"val_loss\"]))\n",
    "                    metric_dev = np.mean(v_val_loss)\n",
    "                    plot_val_loss.append(metric_dev)\n",
    "                    plot_loss.append(np.mean(hist.history[\"loss\"]))\n",
    "\n",
    "                    if metric_dev < bestMetricDev:\n",
    "                        print(\"\\tChange the best \", bestMetricDev, \" by metric dev: \", metric_dev)\n",
    "                        bestMetricDev = metric_dev\n",
    "                        bestHyperparameters['h_layers'] = l\n",
    "                        bestHyperparameters['batch_size'] = bs\n",
    "                        bestHyperparameters['initial_learning_rate'] = init\n",
    "                        bestHyperparameters['dropout'] = d\n",
    "                        print(\"%%%%%%%%%bestHyperparameters%%%%%%%%%\")\n",
    "                        print(bestHyperparameters)\n",
    "\n",
    "                dicc_1[\"bs:\" + str(batch_size[bs]) + \"-lr:\" + str(initial_learning_rate[init])] = [plot_val_loss, plot_loss]\n",
    "\n",
    "    if plotAndSaveImg:\n",
    "        plotLossesLayers(h_layers_1, dicc_1, folders[index_data], dependencia)\n",
    "        \n",
    "    print(\"Layers selected:\", h_layers_1[bestHyperparameters[\"h_layers\"]])\n",
    "    print(\"batch_size selected:\", batch_size[bestHyperparameters[\"batch_size\"]])\n",
    "    print(\"initial_learning_rate selected:\", initial_learning_rate[bestHyperparameters[\"initial_learning_rate\"]])\n",
    "    print(\"dropout selected:\", dropout[bestHyperparameters[\"dropout\"]])\n",
    "\n",
    "    print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%================================================================> BEST MODEL SELECT WITH DIFFERENTS HYPERPARAMETERS <=======================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "    hyperparameters = {'epochs': hyperparameters['epochs'],\n",
    "                   'initial_learning_rate': initial_learning_rate[bestHyperparameters[\"initial_learning_rate\"]],\n",
    "                   \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "                   'batch_size': batch_size[bestHyperparameters[\"batch_size\"]],\n",
    "                   'h_layers': h_layers_1[bestHyperparameters[\"h_layers\"]],\n",
    "                   'dropout': dropout[bestHyperparameters[\"dropout\"]],\n",
    "                   'verbose': 1}\n",
    "    \n",
    "    AE_models.reset_keras()\n",
    "    X_train_aux, X_val_aux = train_test_split(X_pre_train,\n",
    "                                              test_size=0.3,\n",
    "                                              random_state=semillas[index_data])\n",
    "    \n",
    "    autoencoder, hist, early = AE_models.runNetwork(X_train_aux, X_val_aux,\n",
    "                                                    hyperparameters,\n",
    "                                                    autoencodertype)\n",
    "\n",
    "\n",
    "    # Group the layers in an object (input and output)\n",
    "    encoder = keras.Model(autoencoder.input, autoencoder.get_layer('Latent_layer').output)\n",
    "\n",
    "    # Load data\n",
    "    X_train = pd.read_csv('../data_generated_by_dtw/' + dependencia + '/' + folders[index_data] + '/X_train.csv')\n",
    "    X_test = pd.read_csv('../data_generated_by_dtw/' + dependencia + '/' + folders[index_data] + '/X_test.csv')\n",
    "\n",
    "    X_train, X_test = AE_models.normData_minmax(X_train, X_test)\n",
    "    \n",
    "    gamma = 0.8\n",
    "    X_train = np.exp(-gamma*X_train)\n",
    "    X_test = np.exp(-gamma*X_test)\n",
    "\n",
    "    encoder = keras.Model(autoencoder.input, autoencoder.get_layer('Latent_layer').output)\n",
    "    # encode the data\n",
    "    X_train_encode = encoder.predict(X_train)\n",
    "    X_test_encode = encoder.predict(X_test)\n",
    "\n",
    "    pd.DataFrame(X_train_encode).to_csv(\"../1_Clasifications_models/data_reduced/\" + dependencia + \"/AE/X_train_Norm_AE_\" + folders[index_data] + \".csv\", index=False)\n",
    "    pd.DataFrame(X_test_encode).to_csv(\"../1_Clasifications_models/data_reduced/\" + dependencia + \"/AE/X_test_Norm_AE_\" + folders[index_data] + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "normalizar = True\n",
    "plotAndSaveImg = False\n",
    "dependencia = \"DTW_I\"\n",
    "\n",
    "for index_data in range(len(folders)):\n",
    "\n",
    "    print(\"================================================================>\" + folders[index_data] + \"<=======================================================\")\n",
    "    X_train_original_I, X_test_original_I, y_train_original_I, y_test_original_I = AE_models.loadData(index_data, normalizar, folders, dependencia)\n",
    "\n",
    "    # TYPE OF AUTOENCODER\n",
    "    autoencodertype_I = {'DAE': False, 'AE': True}\n",
    "\n",
    "\n",
    "    epochs_I = 5000\n",
    "    initial_learning_rate_I = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    batch_size_I = [32]\n",
    "    dropout_I = [0, .05, .1]\n",
    "\n",
    "    h_layers_I = [[X_train_original_I.shape[0], 450],\n",
    "                  [X_train_original_I.shape[0], 425],\n",
    "                  [X_train_original_I.shape[0], 400],\n",
    "                  [X_train_original_I.shape[0], 375],\n",
    "                  [X_train_original_I.shape[0], 350],\n",
    "                  [X_train_original_I.shape[0], 325],\n",
    "                  [X_train_original_I.shape[0], 300],\n",
    "                  [X_train_original_I.shape[0], 275],\n",
    "                  [X_train_original_I.shape[0], 250]]\n",
    "    \n",
    "    hyperparameters_I = {'epochs': epochs_I,\n",
    "                         'initial_learning_rate': initial_learning_rate_I,\n",
    "                         'kfold': 5,\n",
    "                         \"mindelta\": 0.0001}\n",
    "\n",
    "    y_pre_train_I = y_train_original_I\n",
    "    y_test_I = y_test_original_I\n",
    "    X_pre_train_I = X_train_original_I\n",
    "    X_test_I = X_test_original_I\n",
    "    \n",
    "    gamma_I = 0.8\n",
    "    X_pre_train_I = np.exp(-gamma_I * X_pre_train_I)\n",
    "    X_test_I = np.exp(-gamma_I * X_test_I)\n",
    "\n",
    "    semillas_I = [9, 18, 35, 52, 75]\n",
    "\n",
    "    kf_I = KFold(n_splits=hyperparameters_I[\"kfold\"], shuffle=True, random_state=semillas_I[index_data])\n",
    "    kf_I.get_n_splits(X_pre_train_I)\n",
    "\n",
    "    bestHyperparameters_I = {'h_layers': -1, 'batch_size': -1, 'initial_learning_rate': -1 }\n",
    "\n",
    "    bestMetricDev_I = np.inf\n",
    "    dicc_I = {}\n",
    "    for d in range(len(dropout_I)):\n",
    "        for init in range(len(initial_learning_rate_I)):\n",
    "            for bs in range(len(batch_size_I)):\n",
    "                plot_val_loss_I = []\n",
    "                plot_loss_I = []\n",
    "                for l in range(len(h_layers_I)):\n",
    "                    hyperparameters_aux_I = {'epochs': hyperparameters_I['epochs'],\n",
    "                                             'initial_learning_rate': hyperparameters_I['initial_learning_rate'][init],\n",
    "                                             \"mindelta\": hyperparameters_I[\"mindelta\"],\n",
    "                                             'batch_size': batch_size_I[bs],\n",
    "                                             'h_layers': h_layers_I[l],\n",
    "                                             'dropout': dropout_I[d],\n",
    "                                             'verbose': 0}\n",
    "\n",
    "                    v_early_I = []\n",
    "                    v_metric_dev_I = []\n",
    "                    v_hist_I = []\n",
    "                    v_val_loss_I = []\n",
    "                    for train_index_I, val_index_I in kf_I.split(X_pre_train_I):\n",
    "                        X_train_I, X_val_I = X_pre_train_I[train_index_I], X_pre_train_I[val_index_I]\n",
    "                        # Reset keras\n",
    "                        AE_models.reset_keras()\n",
    "                        \n",
    "                        # Train the network and test it \n",
    "                        model_I, hist_I, early_I = AE_models.runNetwork(X_train_I, X_val_I,\n",
    "                                                                        hyperparameters_aux_I,\n",
    "                                                                        autoencodertype_I)\n",
    "\n",
    "                        v_early_I.append(early_I)\n",
    "                        v_hist_I.append(hist_I)\n",
    "                        v_val_loss_I.append(np.min(hist_I.history[\"val_loss\"]))\n",
    "                    metric_dev_I = np.mean(v_val_loss_I)\n",
    "                    plot_val_loss_I.append(metric_dev_I)\n",
    "                    plot_loss_I.append(np.mean(hist_I.history[\"loss\"]))\n",
    "\n",
    "                    if metric_dev_I < bestMetricDev_I:\n",
    "                        print(\"\\tChange the best \", bestMetricDev_I, \" by metric dev: \", metric_dev_I)\n",
    "                        bestMetricDev_I = metric_dev_I\n",
    "                        bestHyperparameters_I['h_layers'] = l\n",
    "                        bestHyperparameters_I['batch_size'] = bs\n",
    "                        bestHyperparameters_I['initial_learning_rate'] = init\n",
    "                        bestHyperparameters_I['dropout'] = d\n",
    "                        print(\"%%%%%%%%%bestHyperparameters%%%%%%%%%\")\n",
    "                        print(bestHyperparameters_I)\n",
    "\n",
    "                dicc_I[\"bs:\" + str(batch_size_I[bs]) + \"-lr:\" + str(initial_learning_rate_I[init])] = [plot_val_loss_I, plot_loss_I]\n",
    "\n",
    "    if plotAndSaveImg:\n",
    "        plotLossesLayers(h_layers_I, dicc_I, folders[index_data], dependencia)\n",
    "        \n",
    "    print(\"Layers selected:\", h_layers_I[bestHyperparameters_I[\"h_layers\"]])\n",
    "    print(\"batch_size selected:\", batch_size_I[bestHyperparameters_I[\"batch_size\"]])\n",
    "    print(\"initial_learning_rate selected:\", initial_learning_rate_I[bestHyperparameters_I[\"initial_learning_rate\"]])\n",
    "    print(\"dropout selected:\", dropout_I[bestHyperparameters_I[\"dropout\"]])\n",
    "\n",
    "    print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%================================================================> BEST MODEL SELECT WITH DIFFERENTS HYPERPARAMETERS <=======================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "    hyperparameters_I = {'epochs': hyperparameters_I['epochs'],\n",
    "                         'initial_learning_rate': initial_learning_rate_I[bestHyperparameters_I[\"initial_learning_rate\"]],\n",
    "                         \"mindelta\": hyperparameters_I[\"mindelta\"],\n",
    "                         'batch_size': batch_size_I[bestHyperparameters_I[\"batch_size\"]],\n",
    "                         'h_layers': h_layers_I[bestHyperparameters_I[\"h_layers\"]],\n",
    "                         'dropout': dropout_I[bestHyperparameters_I[\"dropout\"]],\n",
    "                         'verbose': 1}\n",
    "    \n",
    "    AE_models.reset_keras()\n",
    "    X_train_aux_I, X_val_aux_I = train_test_split(X_pre_train_I,\n",
    "                                                  test_size=0.3,\n",
    "                                                  random_state=semillas_I[index_data])\n",
    "    \n",
    "    autoencoder_I, hist_I, early_I = AE_models.runNetwork(X_train_aux_I, X_val_aux_I,\n",
    "                                                          hyperparameters_I,\n",
    "                                                          autoencodertype_I)\n",
    "\n",
    "\n",
    "    # Group the layers in an object (input and output)\n",
    "    encoder_I = keras.Model(autoencoder_I.input, autoencoder_I.get_layer('Latent_layer').output)\n",
    "\n",
    "    # Load data\n",
    "    X_train_I = pd.read_csv('../data_generated_by_dtw/' + dependencia + '/' + folders[index_data] + '/X_train.csv')\n",
    "    X_test_I = pd.read_csv('../data_generated_by_dtw/' + dependencia + '/' + folders[index_data] + '/X_test.csv')\n",
    "\n",
    "    X_train_I, X_test_I = AE_models.normData_minmax(X_train_I, X_test_I)\n",
    "    \n",
    "    gamma_I = 0.8\n",
    "    X_train_I = np.exp(-gamma_I * X_train_I)\n",
    "    X_test_I = np.exp(-gamma_I * X_test_I)\n",
    "\n",
    "    encoder_I = keras.Model(autoencoder_I.input, autoencoder_I.get_layer('Latent_layer').output)\n",
    "    # encode the data\n",
    "    X_train_encode_I = encoder_I.predict(X_train_I)\n",
    "    X_test_encode_I = encoder_I.predict(X_test_I)\n",
    "\n",
    "    pd.DataFrame(X_train_encode_I).to_csv(\"../1_Clasifications_models/data_reduced/\" + dependencia + \"/AE/X_train_Norm_AE_\" + folders[index_data] + \".csv\", index=False)\n",
    "    pd.DataFrame(X_test_encode_I).to_csv(\"../1_Clasifications_models/data_reduced/\" + dependencia + \"/AE/X_test_Norm_AE_\" + folders[index_data] + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
