{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import AE_models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras \n",
    "from keras import layers\n",
    "import scipy.io as sio\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_range_analysis(X):\n",
    "    if sum(sum(((np.round(X,4) <= 1)) & (X >= 0))) == X.shape[0] * X.shape[1]:\n",
    "        print(\"Range value: [0,1]\")\n",
    "    else:\n",
    "        print(\"ERROR!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLossesLayers(h_layers, dicc, index_data, data):\n",
    "    plot_layers = []\n",
    "    for i in range(len(h_layers)):\n",
    "        plot_layers.append(h_layers[i][1])\n",
    "\n",
    "    keys = list(dicc.keys())\n",
    "    print(len(keys))\n",
    "    \n",
    "    filas = int(np.ceil(len(keys)/2))\n",
    "    columnas = int(np.sqrt(np.round(len(keys)/2)))\n",
    "    print(filas)\n",
    "    print(columnas)\n",
    "    f, axis = plt.subplots(filas, columnas, figsize=(20,20))\n",
    "\n",
    "    index_keys = 0\n",
    "    for i in range(filas):\n",
    "        for j in range(columnas):\n",
    "            # error in validation\n",
    "            axis[i,j].plot(plot_layers, dicc[keys[index_keys]][0])\n",
    "            # error in train\n",
    "            axis[i,j].plot(plot_layers, dicc[keys[index_keys]][1])\n",
    "            axis[i,j].legend([\"val_loss\", \"loss\"])\n",
    "            axis[i,j].set_title(keys[i])\n",
    "            index_keys += 1\n",
    "\n",
    "    plt.close(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folders = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "normalizar = True\n",
    "plotAndSaveImg_train = False\n",
    "typeData = \"FE\"\n",
    "\n",
    "for index_data in range(len(folders)):\n",
    "    print(\"================================================================>\" + folders[index_data] + \"<=======================================================\")\n",
    "    X_train_original, X_test_original, y_train_original, y_test_original = AE_models.loadData(index_data, normalizar, typeData, folders)\n",
    "    value_range_analysis(X_train_original)\n",
    "    \n",
    "    autoencodertype = {'DAE': False, 'AE': True}\n",
    "    \n",
    "    epochs = 5000\n",
    "    initial_learning_rate = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    dropout=[0, .05, .1]\n",
    "    batch_size = [32]\n",
    "    h_layers_2 = [[X_train_original.shape[1], 140],\n",
    "                [X_train_original.shape[1], 135], \n",
    "                [X_train_original.shape[1], 130],\n",
    "                [X_train_original.shape[1], 125],\n",
    "                [X_train_original.shape[1], 120],\n",
    "                [X_train_original.shape[1], 115],\n",
    "                [X_train_original.shape[1], 110],\n",
    "                [X_train_original.shape[1], 100]]\n",
    "\n",
    "\n",
    "    hyperparameters = {'epochs': epochs,\n",
    "                       'initial_learning_rate':initial_learning_rate,\n",
    "                       'kfold':5,\n",
    "                       \"mindelta\": 0}\n",
    "\n",
    "    y_pre_train = y_train_original\n",
    "    y_test = y_test_original\n",
    "    X_pre_train = X_train_original\n",
    "    X_test = X_test_original\n",
    "\n",
    "    semillas = [9,18,35, 52, 75]\n",
    "\n",
    "    kf = KFold(n_splits=hyperparameters[\"kfold\"], shuffle=True, random_state=semillas[index_data])\n",
    "    kf.get_n_splits(X_pre_train)\n",
    "\n",
    "    bestHyperparameters_2 = {'h_layers': -1, 'batch_size': -1, 'initial_learning_rate': -1 }\n",
    "\n",
    "    bestMetricDev = np.inf\n",
    "\n",
    "    dicc_2 = {}\n",
    "    for init in range(len(initial_learning_rate)):\n",
    "        for bs in range(len(batch_size)):\n",
    "            for d in range(len(dropout)):\n",
    "                plot_val_loss_mean = []\n",
    "                plot_loss_mean = []\n",
    "                for l in range(len(h_layers_2)):\n",
    "                    hyperparameters_aux = {'epochs': hyperparameters['epochs'],\n",
    "                                       'initial_learning_rate': hyperparameters['initial_learning_rate'][init],\n",
    "                                       \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "                                       'batch_size': batch_size[bs],\n",
    "                                       'dropout': dropout[d],\n",
    "                                       'h_layers': h_layers_2[l], 'verbose': 0}\n",
    "\n",
    "                    v_metric_dev = []\n",
    "                    val_loss = []\n",
    "                    loss = []\n",
    "                    for train_index, val_index in kf.split(X_pre_train):\n",
    "                        X_train, X_val = X_pre_train[train_index], X_pre_train[val_index]\n",
    "                        # Reset keras\n",
    "                        AE_models.reset_keras()\n",
    "                        # Train the network and test it\n",
    "                        model, hist, early = AE_models.runNetwork(X_train, X_val,\n",
    "                                                                  hyperparameters_aux,\n",
    "                                                                  autoencodertype)\n",
    "\n",
    "\n",
    "                        loss.append(np.min(hist.history[\"loss\"]))\n",
    "                        val_loss.append(np.min(hist.history[\"val_loss\"]))\n",
    "\n",
    "                    plot_val_loss_mean.append(np.mean(val_loss))\n",
    "                    plot_loss_mean.append(np.mean(loss))\n",
    "\n",
    "                    metric_dev = np.mean(val_loss)\n",
    "                    print(\"%%%%%%%%%bestHyperparameters%%%%%%%%%\")\n",
    "                    print(bestHyperparameters_2)\n",
    "\n",
    "                    if metric_dev < bestMetricDev:\n",
    "                        print(\"\\tChange the best \", bestMetricDev, \" by metric dev: \", metric_dev)\n",
    "                        bestMetricDev = metric_dev\n",
    "                        bestHyperparameters_2['h_layers'] = l\n",
    "                        bestHyperparameters_2['batch_size'] = bs\n",
    "                        bestHyperparameters_2['initial_learning_rate'] = init\n",
    "                        bestHyperparameters_2['dropout'] = d\n",
    "                \n",
    "                dicc_2[\"bs:\" + str(batch_size[bs]) + \"-lr:\" + str(initial_learning_rate[init]) + \"-drop:\" + str(dropout[d])] = [plot_val_loss_mean, plot_loss_mean]\n",
    "\n",
    "    if plotAndSaveImg_train:\n",
    "        plotLossesLayers(h_layers_2, dicc_2, index_data, typeData)\n",
    "\n",
    "    print(\"Layers selected:\", h_layers_2[bestHyperparameters_2[\"h_layers\"]])\n",
    "    print(\"dropout selected:\", dropout[bestHyperparameters_2[\"dropout\"]])\n",
    "    print(\"batch_size selected:\", batch_size[bestHyperparameters_2[\"batch_size\"]])\n",
    "    print(\"initial_learning_rate selected:\", initial_learning_rate[bestHyperparameters_2[\"initial_learning_rate\"]])\n",
    "\n",
    "    print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%================================================================> BEST MODEL SELECT WITH DIFFERENTS HYPERPARAMETERS <=======================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "    hyperparameters = {'epochs': hyperparameters['epochs'],\n",
    "                   'initial_learning_rate': initial_learning_rate[bestHyperparameters_2[\"initial_learning_rate\"]],\n",
    "                   \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "                   'batch_size': batch_size[bestHyperparameters_2[\"batch_size\"]],\n",
    "                   'h_layers': h_layers_2[bestHyperparameters_2[\"h_layers\"]],\n",
    "                   'dropout': dropout[bestHyperparameters_2[\"dropout\"]],\n",
    "                   'verbose': 1}\n",
    "    \n",
    "    X_train_aux, X_val_aux = train_test_split(X_pre_train,\n",
    "                                              test_size=0.3,\n",
    "                                              random_state=semillas[index_data])\n",
    "    \n",
    "    autoencoder, hist, early = AE_models.runNetwork(X_train_aux, X_val_aux,\n",
    "                                                    hyperparameters,\n",
    "                                                    autoencodertype)\n",
    "\n",
    "    encoder_2 = keras.Model(autoencoder.input, autoencoder.get_layer('Latent_layer').output)\n",
    "    # Load the data\n",
    "    X_train = pd.read_csv('../../data_generated_by_statistics/' + typeData + '/' + folders[index_data] + '/X_train.csv')\n",
    "    X_test = pd.read_csv('../../data_generated_by_statistics/' + typeData + '/' + folders[index_data] + '/X_test.csv')\n",
    "    \n",
    "    X_train, X_test = AE_models.normData_minmax(X_train, X_test)\n",
    "\n",
    "    encoder_2 = keras.Model(autoencoder.input, autoencoder.get_layer('Latent_layer').output)\n",
    "    # Encode the data\n",
    "    X_train_encode_2 = encoder_2.predict(X_train)\n",
    "    X_test_encode_2 = encoder_2.predict(X_test)\n",
    "\n",
    "    pd.DataFrame(X_train_encode_2).to_csv(\"../../1_Clasifications_models/data_reduced/\"+ typeData +\"/AE/X_train_Norm_AE_\" + folders[index_data] + \".csv\", index=False)\n",
    "    pd.DataFrame(X_test_encode_2).to_csv(\"../../1_Clasifications_models/data_reduced/\"+ typeData +\"/AE/X_test_Norm_AE_\" + folders[index_data] + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FE_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "normalizar = True\n",
    "plotAndSaveImg_train = False\n",
    "typeData = \"FE_kernel\"\n",
    "\n",
    "for index_data in range(len(folders)):\n",
    "    print(\"================================================================>\" + folders[index_data] + \"<=======================================================\")\n",
    "    X_train_original, X_test_original, y_train_original, y_test_original = AE_models.loadData(index_data, normalizar, typeData, folders)\n",
    "    value_range_analysis(X_train_original)\n",
    "    \n",
    "    autoencodertype = {'DAE': False, 'AE': True}\n",
    "    \n",
    "    epochs = 5000\n",
    "    initial_learning_rate = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    dropout=[0, .05, .1]\n",
    "    batch_size = [32]\n",
    "    h_layers_3 = [[X_train_original.shape[0], 450],\n",
    "                [X_train_original.shape[0], 425],\n",
    "                [X_train_original.shape[0], 400],\n",
    "                [X_train_original.shape[0], 375],\n",
    "                [X_train_original.shape[0],  350],\n",
    "                [X_train_original.shape[0],  325],\n",
    "                [X_train_original.shape[0],  300],\n",
    "                [X_train_original.shape[0], 275],\n",
    "                [X_train_original.shape[0], 250],\n",
    "                [X_train_original.shape[0], 225],\n",
    "                [X_train_original.shape[0], 200]]\n",
    "\n",
    "    hyperparameters = {'epochs': epochs,\n",
    "                       'initial_learning_rate':initial_learning_rate,\n",
    "                       'kfold':5,\n",
    "                       \"mindelta\": 0.0001}\n",
    "\n",
    "    y_pre_train = y_train_original\n",
    "    y_test = y_test_original\n",
    "    X_pre_train = X_train_original\n",
    "    X_test = X_test_original\n",
    "\n",
    "    semillas = [9,18,35, 52, 75]\n",
    "\n",
    "    kf = KFold(n_splits=hyperparameters[\"kfold\"], shuffle=True, random_state=semillas[index_data])\n",
    "    kf.get_n_splits(X_pre_train)\n",
    "\n",
    "    bestHyperparameters_3 = {'h_layers': -1, 'batch_size': -1, 'initial_learning_rate': -1 }\n",
    "\n",
    "    bestMetricDev = np.inf\n",
    "\n",
    "    dicc_3 = {}\n",
    "    for init in range(len(initial_learning_rate)):\n",
    "        for bs in range(len(batch_size)):\n",
    "            for d in range(len(dropout)):\n",
    "                plot_val_loss_mean = []\n",
    "                plot_loss_mean = []\n",
    "                for l in range(len(h_layers_3)):\n",
    "                    hyperparameters_aux = {'epochs': hyperparameters['epochs'],\n",
    "                                       'initial_learning_rate': hyperparameters['initial_learning_rate'][init],\n",
    "                                       \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "                                       'batch_size': batch_size[bs],\n",
    "                                       'dropout': dropout[d],\n",
    "                                       'h_layers': h_layers_3[l], 'verbose': 0}\n",
    "\n",
    "                    v_metric_dev = []\n",
    "                    val_loss = []\n",
    "                    loss = []\n",
    "                    for train_index, val_index in kf.split(X_pre_train):\n",
    "                        X_train, X_val = X_pre_train[train_index], X_pre_train[val_index]\n",
    "                        # Reset keras\n",
    "                        AE_models.reset_keras()\n",
    "                        # Train the network and test it\n",
    "                        model, hist, early = AE_models.runNetwork(X_train, X_val,\n",
    "                                                                  hyperparameters_aux,\n",
    "                                                                  autoencodertype)\n",
    "\n",
    "\n",
    "                        loss.append(np.min(hist.history[\"loss\"]))\n",
    "                        val_loss.append(np.min(hist.history[\"val_loss\"]))\n",
    "\n",
    "                    plot_val_loss_mean.append(np.mean(val_loss))\n",
    "                    plot_loss_mean.append(np.mean(loss))\n",
    "\n",
    "                    metric_dev = np.mean(val_loss)\n",
    "\n",
    "                    if metric_dev < bestMetricDev:\n",
    "                        print(\"\\tChange the best \", bestMetricDev, \"by metric dev: \", metric_dev)\n",
    "                        bestMetricDev = metric_dev\n",
    "                        bestHyperparameters_3['h_layers'] = l\n",
    "                        bestHyperparameters_3['batch_size'] = bs\n",
    "                        bestHyperparameters_3['initial_learning_rate'] = init\n",
    "                        bestHyperparameters_3['dropout'] = d\n",
    "                        print(\"%%%%%%%%%bestHyperparameters%%%%%%%%%\")\n",
    "                        print(bestHyperparameters_3)\n",
    "\n",
    "                dicc_3[\"bs:\" + str(batch_size[bs]) + \"-lr:\" + str(initial_learning_rate[init]) + \"-drop:\" + str(dropout[d])] = [plot_val_loss_mean, plot_loss_mean]\n",
    "\n",
    "    if plotAndSaveImg_train:\n",
    "        plotLossesLayers(h_layers_3, dicc_3, index_data, typeData)\n",
    "\n",
    "    print(\"Layers selected:\", h_layers_3[bestHyperparameters_3[\"h_layers\"]])\n",
    "    print(\"dropout selected:\", dropout[bestHyperparameters_3[\"dropout\"]])\n",
    "    print(\"batch_size selected:\", batch_size[bestHyperparameters_3[\"batch_size\"]])\n",
    "    print(\"initial_learning_rate selected:\", initial_learning_rate[bestHyperparameters_3[\"initial_learning_rate\"]])\n",
    "\n",
    "    print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%================================================================> BEST MODEL SELECT WITH DIFFERENTS HYPERPARAMETERS <=======================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "    hyperparameters = {'epochs': hyperparameters['epochs'],\n",
    "                   'initial_learning_rate': initial_learning_rate[bestHyperparameters_3[\"initial_learning_rate\"]],\n",
    "                   \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "                   'batch_size': batch_size[bestHyperparameters_3[\"batch_size\"]],\n",
    "                   'h_layers': h_layers_3[bestHyperparameters_3[\"h_layers\"]],\n",
    "                   'dropout': dropout[bestHyperparameters_3[\"dropout\"]],\n",
    "                   'verbose': 1}\n",
    "    \n",
    "    X_train_aux, X_val_aux = train_test_split(X_pre_train,\n",
    "                                              test_size=0.3,\n",
    "                                              random_state=semillas[index_data])\n",
    "    \n",
    "    autoencoder, hist, early = AE_models.runNetwork(X_train_aux, X_val_aux,\n",
    "                                                    hyperparameters,\n",
    "                                                    autoencodertype)\n",
    "\n",
    "    encoder_3 = keras.Model(autoencoder.input, autoencoder.get_layer('Latent_layer').output)\n",
    "    # Load the data\n",
    "    X_train = pd.read_csv('../../data_generated_by_statistics/' + typeData + '/' + folders[index_data] + '/X_train.csv')\n",
    "    X_test = pd.read_csv('../../data_generated_by_statistics/' + typeData + '/' + folders[index_data] + '/X_test.csv')\n",
    "\n",
    "    X_train, X_test = AE_models.normData_minmax(X_train, X_test)\n",
    "\n",
    "    encoder_3 = keras.Model(autoencoder.input, autoencoder.get_layer('Latent_layer').output)\n",
    "    # Encode the data\n",
    "    X_train_encode_3 = encoder_3.predict(X_train)\n",
    "    X_test_encode_3 = encoder_3.predict(X_test)\n",
    "\n",
    "    pd.DataFrame(X_train_encode_3).to_csv(\"../../1_Clasifications_models/data_reduced/\"+ typeData +\"/AE/X_train_Norm_AE_\" + folders[index_data] + \".csv\", index=False)\n",
    "    pd.DataFrame(X_test_encode_3).to_csv(\"../../1_Clasifications_models/data_reduced/\"+ typeData +\"/AE/X_test_Norm_AE_\" + folders[index_data] + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
