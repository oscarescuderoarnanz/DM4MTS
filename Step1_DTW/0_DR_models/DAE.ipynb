{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import AE_models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras \n",
    "from keras import layers\n",
    "import scipy.io as sio\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLossesLayers(h_layers, dicc, carpeta, dependencia):\n",
    "    plot_layers = []\n",
    "    for i in range(len(h_layers)):\n",
    "        plot_layers.append(h_layers[i][1])\n",
    "\n",
    "    keys = list(dicc.keys())\n",
    "\n",
    "    filas = int(np.sqrt(np.ceil(len(keys)/2)))*3\n",
    "    columnas = int(np.sqrt(np.round(len(keys)/2)))\n",
    "    \n",
    "    f, axis = plt.subplots(filas, columnas, figsize=(20,20))\n",
    "    index_keys = 0\n",
    "    for i in range(filas):\n",
    "        for j in range(columnas):\n",
    "            # error in validation\n",
    "            axis[i,j].plot(plot_layers, dicc[keys[index_keys]][0])\n",
    "            # error in train\n",
    "            axis[i,j].plot(plot_layers, dicc[keys[index_keys]][1])\n",
    "            axis[i,j].legend([\"val_loss\", \"loss\"])\n",
    "            axis[i,j].set_title(keys[i])\n",
    "            index_keys += 1\n",
    "\n",
    "    plt.close(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "normalizar = True\n",
    "plotAndSaveImg = False\n",
    "dependencia = \"DTW_D\"\n",
    "\n",
    "errores1 = []\n",
    "errores2 = []\n",
    "\n",
    "for index_data in range(len(folders)):\n",
    "\n",
    "    print(\"================================================================>\" + folders[index_data] + \"<=======================================================\")\n",
    "    X_train_original, X_test_original, y_train_original, y_test_original = AE_models.loadData(index_data, normalizar, folders, dependencia)\n",
    "    \n",
    "    autoencodertype = {'DAE': True, 'AE': False}\n",
    "\n",
    "    epochs = 5000\n",
    "    initial_learning_rate = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    batch_size = [32]\n",
    "    dropout=[0, .05, .1]\n",
    "    std_noise = [0.01, 0.025, 0.05]\n",
    "    h_layers_1 = [[X_train_original.shape[0], 450],\n",
    "                [X_train_original.shape[0], 425],\n",
    "                [X_train_original.shape[0], 400],\n",
    "                [X_train_original.shape[0], 375],\n",
    "                [X_train_original.shape[0],  350],\n",
    "                [X_train_original.shape[0],  325],\n",
    "                [X_train_original.shape[0],  300],\n",
    "                [X_train_original.shape[0], 275],\n",
    "                [X_train_original.shape[0], 250],\n",
    "                [X_train_original.shape[0], 225],\n",
    "                [X_train_original.shape[0], 200]]\n",
    "\n",
    "\n",
    "    hyperparameters = {'epochs': epochs,\n",
    "                       'initial_learning_rate':initial_learning_rate,\n",
    "                       'kfold':5,\n",
    "                       \"mindelta\": 0.0001}\n",
    "\n",
    "    y_pre_train = y_train_original\n",
    "    y_test = y_test_original\n",
    "    X_pre_train = X_train_original\n",
    "    X_test = X_test_original\n",
    "\n",
    "    semillas = [9,18,35, 52, 75]\n",
    "\n",
    "    kf = KFold(n_splits=hyperparameters[\"kfold\"], shuffle=True, random_state=semillas[index_data])\n",
    "    kf.get_n_splits(X_pre_train)\n",
    "\n",
    "    bestHyperparameters = {'h_layers': -1, 'batch_size': -1, 'initial_learning_rate': -1 }\n",
    "\n",
    "    bestMetricDev = np.inf\n",
    "    dicc_1 = {}\n",
    "\n",
    "    for init in range(len(initial_learning_rate)):\n",
    "        for std in range(len(std_noise)):\n",
    "            for bs in range(len(batch_size)):\n",
    "                for d in range(len(dropout)):\n",
    "                    plot_val_loss = []\n",
    "                    plot_loss = []\n",
    "                    for l in range(len(h_layers_1)):\n",
    "                        hyperparameters_aux = {'epochs': hyperparameters['epochs'],\n",
    "                                           'initial_learning_rate': hyperparameters['initial_learning_rate'][init],\n",
    "                                           \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "                                           'batch_size': batch_size[bs],\n",
    "                                           'h_layers': h_layers_1[l],\n",
    "                                           'dropout': dropout[d],\n",
    "                                           'std_noise': std_noise[std],\n",
    "                                           'verbose': 0}\n",
    "\n",
    "                        v_early = []\n",
    "                        v_metric_dev = []\n",
    "                        v_hist = []\n",
    "                        v_val_loss = []\n",
    "                        for train_index, val_index in kf.split(X_pre_train):\n",
    "                            X_train, X_val = X_pre_train[train_index], X_pre_train[val_index]\n",
    "                            # Reset keras\n",
    "                            AE_models.reset_keras()\n",
    "                            # Train the network and test it\n",
    "                            model, hist, early = AE_models.runNetwork(X_train, X_val,\n",
    "                                                                      hyperparameters_aux,\n",
    "                                                                      autoencodertype)\n",
    "\n",
    "                            v_early.append(early)\n",
    "                            v_hist.append(hist)\n",
    "                            v_val_loss.append(np.min(hist.history[\"val_loss\"]))\n",
    "                        metric_dev = np.mean(v_val_loss)\n",
    "                        plot_val_loss.append(metric_dev)\n",
    "                        plot_loss.append(np.mean(hist.history[\"loss\"]))\n",
    "\n",
    "                        if metric_dev < bestMetricDev:\n",
    "                            print(\"\\tChange the best \", bestMetricDev, \" by metric dev: \", metric_dev)\n",
    "                            bestMetricDev = metric_dev\n",
    "                            bestHyperparameters['h_layers'] = l\n",
    "                            bestHyperparameters['batch_size'] = bs\n",
    "                            bestHyperparameters['initial_learning_rate'] = init\n",
    "                            bestHyperparameters['std_noise'] = std\n",
    "                            bestHyperparameters['dropout'] = d\n",
    "                            print(\"%%%%%%%%%bestHyperparameters%%%%%%%%%\")\n",
    "                            print(bestHyperparameters)\n",
    "\n",
    "                    dicc_1[\"bs:\" + str(batch_size[bs]) + \"-lr:\" + str(initial_learning_rate[init]) + \"-std:\" + str(std_noise[std])] = [plot_val_loss, plot_loss]\n",
    "\n",
    "    if plotAndSaveImg:\n",
    "        plotLossesLayers(h_layers_1, dicc_1, folders[index_data], dependencia)\n",
    "        \n",
    "    print(\"Layers selected:\", h_layers[bestHyperparameters[\"h_layers\"]])\n",
    "    print(\"batch_size selected:\", batch_size[bestHyperparameters[\"batch_size\"]])\n",
    "    print(\"initial_learning_rate selected:\", initial_learning_rate[bestHyperparameters[\"initial_learning_rate\"]])\n",
    "    print(\"std_noise selected:\", std_noise[bestHyperparameters[\"std_noise\"]])\n",
    "    print(\"dropout selected:\", dropout[bestHyperparameters[\"dropout\"]])\n",
    "\n",
    "    print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%================================================================> BEST MODEL SELECT WITH DIFFERENTS HYPERPARAMETERS <=======================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "    hyperparameters = {'epochs': hyperparameters['epochs'],\n",
    "                   'initial_learning_rate': initial_learning_rate[bestHyperparameters[\"initial_learning_rate\"]],\n",
    "                   \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "                   'batch_size': batch_size[bestHyperparameters[\"batch_size\"]],\n",
    "                   'h_layers': h_layers_1[bestHyperparameters[\"h_layers\"]],\n",
    "                   'std_noise': std_noise[bestHyperparameters[\"std_noise\"]],\n",
    "                   'dropout': dropout[bestHyperparameters[\"dropout\"]],\n",
    "                   'verbose': 1}\n",
    "    \n",
    "    AE_models.reset_keras()\n",
    "    X_train_aux, X_val_aux = train_test_split(X_pre_train,\n",
    "                                              test_size=0.3,\n",
    "                                              random_state=semillas[index_data])\n",
    "    \n",
    "    autoencoder, hist, early = AE_models.runNetwork(X_train_aux, X_val_aux,\n",
    "                                                    hyperparameters,\n",
    "                                                    autoencodertype)\n",
    "    \n",
    "    denominador = 0\n",
    "    X_val_aux = pd.DataFrame(X_val_aux)\n",
    "    for i in range(X_val_aux.shape[1]):\n",
    "        denominador += np.mean(np.square(X_val_aux.iloc[:, i].values))\n",
    "    \n",
    "    errores1.append((hist.history[\"val_loss\"][-2]/((1/X_val_aux.shape[1])*denominador))*100)\n",
    "    errores2.append((hist.history[\"val_loss\"][-2]/((1/X_val_aux.shape[0])*denominador))*100)\n",
    "\n",
    "\n",
    "    # Group the layers in an object (input and output)\n",
    "    encoder = keras.Model(autoencoder.input, autoencoder.get_layer('Latent_layer').output)\n",
    "\n",
    "    X_train = pd.read_csv('../data_generated_by_dtw/' + dependencia + '/' + folders[index_data] + '/X_train.csv')\n",
    "    X_test = pd.read_csv('../data_generated_by_dtw/' + dependencia + '/' + folders[index_data] + '/X_test.csv')\n",
    "\n",
    "    X_train, X_test = AE_models.normData_minmax(X_train, X_test)\n",
    "\n",
    "    encoder = keras.Model(autoencoder.input, autoencoder.get_layer('Latent_layer').output)\n",
    "    #encode the data\n",
    "    X_train_encode = encoder.predict(X_train)\n",
    "    X_test_encode = encoder.predict(X_test)\n",
    "\n",
    "    pd.DataFrame(X_train_encode).to_csv(\"../1_Clasifications_models/data_reduced/\" + dependencia + \"/DAE/X_train_Norm_DAE_\" + folders[index_data] + \".csv\", index=False)\n",
    "    pd.DataFrame(X_test_encode).to_csv(\"../1_Clasifications_models/data_reduced/\" + dependencia + \"/DAE/X_test_Norm_DAE_\" + folders[index_data] + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_I = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "normalizar_I = True\n",
    "plotAndSaveImg_I = False\n",
    "dependencia_I = \"DTW_I\"\n",
    "\n",
    "for index_data_I in range(len(folders_I)):\n",
    "\n",
    "    print(\"================================================================>\" + folders_I[index_data_I] + \"<=======================================================\")\n",
    "    X_train_original_I, X_test_original_I, y_train_original_I, y_test_original_I = AE_models.loadData(index_data_I, normalizar_I, folders_I, dependencia_I)\n",
    "\n",
    "    autoencodertype_I = {'DAE': True, 'AE': False}\n",
    "\n",
    "    epochs_I = 5000\n",
    "    initial_learning_rate_I = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    batch_size_I = [32]\n",
    "    dropout_I = [0, .05, .1]\n",
    "    std_noise_I = [0.01, 0.025, 0.05]\n",
    "    h_layers_I = [[X_train_original_I.shape[0], 450],\n",
    "                [X_train_original_I.shape[0], 425],\n",
    "                [X_train_original_I.shape[0], 400],\n",
    "                [X_train_original_I.shape[0], 375],\n",
    "                [X_train_original_I.shape[0],  350],\n",
    "                [X_train_original_I.shape[0],  325],\n",
    "                [X_train_original_I.shape[0],  300],\n",
    "                [X_train_original_I.shape[0], 275],\n",
    "                [X_train_original_I.shape[0], 250],\n",
    "                [X_train_original_I.shape[0], 225],\n",
    "                [X_train_original_I.shape[0], 200]]\n",
    "\n",
    "\n",
    "    hyperparameters_I = {'epochs': epochs_I,\n",
    "                         'initial_learning_rate': initial_learning_rate_I,\n",
    "                         'kfold': 5,\n",
    "                         \"mindelta\": 0.0001}\n",
    "\n",
    "    y_pre_train_I = y_train_original_I\n",
    "    y_test_I = y_test_original_I\n",
    "    X_pre_train_I = X_train_original_I\n",
    "    X_test_I = X_test_original_I\n",
    "\n",
    "    semillas_I = [9, 18, 35, 52, 75]\n",
    "\n",
    "    kf_I = KFold(n_splits=hyperparameters_I[\"kfold\"], shuffle=True, random_state=semillas_I[index_data_I])\n",
    "    kf_I.get_n_splits(X_pre_train_I)\n",
    "\n",
    "    bestHyperparameters_I = {'h_layers': -1, 'batch_size': -1, 'initial_learning_rate': -1 }\n",
    "\n",
    "    bestMetricDev_I = np.inf\n",
    "    dicc_I = {}\n",
    "\n",
    "    for init_I in range(len(initial_learning_rate_I)):\n",
    "        for std_I in range(len(std_noise_I)):\n",
    "            for bs_I in range(len(batch_size_I)):\n",
    "                for d_I in range(len(dropout_I)):\n",
    "                    plot_val_loss_I = []\n",
    "                    plot_loss_I = []\n",
    "                    for l_I in range(len(h_layers_I)):\n",
    "                        hyperparameters_aux_I = {'epochs': hyperparameters_I['epochs'],\n",
    "                                                 'initial_learning_rate': hyperparameters_I['initial_learning_rate'][init_I],\n",
    "                                                 \"mindelta\": hyperparameters_I[\"mindelta\"],\n",
    "                                                 'batch_size': batch_size_I[bs_I],\n",
    "                                                 'h_layers': h_layers_I[l_I],\n",
    "                                                 'dropout': dropout_I[d_I],\n",
    "                                                 'std_noise': std_noise_I[std_I],\n",
    "                                                 'verbose': 0}\n",
    "\n",
    "                        v_early_I = []\n",
    "                        v_metric_dev_I = []\n",
    "                        v_hist_I = []\n",
    "                        v_val_loss_I = []\n",
    "                        for train_index_I, val_index_I in kf_I.split(X_pre_train_I):\n",
    "                            X_train_I, X_val_I = X_pre_train_I[train_index_I], X_pre_train_I[val_index_I]\n",
    "                            # Reset keras\n",
    "                            AE_models.reset_keras()\n",
    "                            # Train the network and test it\n",
    "                            model_I, hist_I, early_I = AE_models.runNetwork(X_train_I, X_val_I,\n",
    "                                                                      hyperparameters_aux_I,\n",
    "                                                                      autoencodertype_I)\n",
    "\n",
    "                            v_early_I.append(early_I)\n",
    "                            v_hist_I.append(hist_I)\n",
    "                            v_val_loss_I.append(np.min(hist_I.history[\"val_loss\"]))\n",
    "                        metric_dev_I = np.mean(v_val_loss_I)\n",
    "                        plot_val_loss_I.append(metric_dev_I)\n",
    "                        plot_loss_I.append(np.mean(hist_I.history[\"loss\"]))\n",
    "\n",
    "                        if metric_dev_I < bestMetricDev_I:\n",
    "                            print(\"\\tChange the best \", bestMetricDev_I, \" by metric dev: \", metric_dev_I)\n",
    "                            bestMetricDev_I = metric_dev_I\n",
    "                            bestHyperparameters_I['h_layers'] = l_I\n",
    "                            bestHyperparameters_I['batch_size'] = bs_I\n",
    "                            bestHyperparameters_I['initial_learning_rate'] = init_I\n",
    "                            bestHyperparameters_I['std_noise'] = std_I\n",
    "                            bestHyperparameters_I['dropout'] = d_I\n",
    "                            print(\"%%%%%%%%%bestHyperparameters%%%%%%%%%\")\n",
    "                            print(bestHyperparameters_I)\n",
    "\n",
    "                    dicc_I[\"bs:\" + str(batch_size_I[bs_I]) + \"-lr:\" + str(initial_learning_rate_I[init_I]) + \"-std:\" + str(std_noise_I[std_I])] = [plot_val_loss_I, plot_loss_I]\n",
    "\n",
    "    if plotAndSaveImg_I:\n",
    "        plotLossesLayers(h_layers_I, dicc_I, folders_I[index_data_I], dependencia_I)\n",
    "        \n",
    "    print(\"Layers selected:\", h_layers_I[bestHyperparameters_I[\"h_layers\"]])\n",
    "    print(\"batch_size selected:\", batch_size_I[bestHyperparameters_I[\"batch_size\"]])\n",
    "    print(\"initial_learning_rate selected:\", initial_learning_rate_I[bestHyperparameters_I[\"initial_learning_rate\"]])\n",
    "    print(\"std_noise selected:\", std_noise_I[bestHyperparameters_I[\"std_noise\"]])\n",
    "    print(\"dropout selected:\", dropout_I[bestHyperparameters_I[\"dropout\"]])\n",
    "\n",
    "    print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%================================================================> BEST MODEL SELECT WITH DIFFERENTS HYPERPARAMETERS <=======================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "    hyperparameters_I = {'epochs': hyperparameters_I['epochs'],\n",
    "                         'initial_learning_rate': initial_learning_rate_I[bestHyperparameters_I[\"initial_learning_rate\"]],\n",
    "                         \"mindelta\": hyperparameters_I[\"mindelta\"],\n",
    "                         'batch_size': batch_size_I[bestHyperparameters_I[\"batch_size\"]],\n",
    "                         'h_layers': h_layers_I[bestHyperparameters_I[\"h_layers\"]],\n",
    "                         'std_noise': std_noise_I[bestHyperparameters_I[\"std_noise\"]],\n",
    "                         'dropout': dropout_I[bestHyperparameters_I[\"dropout\"]],\n",
    "                         'verbose': 1}\n",
    "    \n",
    "    AE_models.reset_keras()\n",
    "    X_train_aux_I, X_val_aux_I = train_test_split(X_pre_train_I,\n",
    "                                                  test_size=0.3,\n",
    "                                                  random_state=semillas_I[index_data_I])\n",
    "    \n",
    "    autoencoder_I, hist_I, early_I = AE_models.runNetwork(X_train_aux_I, X_val_aux_I,\n",
    "                                                          hyperparameters_I,\n",
    "                                                          autoencodertype_I)\n",
    "\n",
    "\n",
    "    # Group the layers in an object (input and output)\n",
    "    encoder_I = keras.Model(autoencoder_I.input, autoencoder_I.get_layer('Latent_layer').output)\n",
    "\n",
    "    X_train_I = pd.read_csv('../data_generated_by_dtw/' + dependencia_I + '/' + folders_I[index_data_I] + '/X_train.csv')\n",
    "    X_test_I = pd.read_csv('../data_generated_by_dtw/' + dependencia_I + '/' + folders_I[index_data_I] + '/X_test.csv')\n",
    "\n",
    "    X_train_I, X_test_I = AE_models.normData_minmax(X_train_I, X_test_I)\n",
    "\n",
    "    encoder_I = keras.Model(autoencoder_I.input, autoencoder_I.get_layer('Latent_layer').output)\n",
    "    # encode the data\n",
    "    X_train_encode_I = encoder_I.predict(X_train_I)\n",
    "    X_test_encode_I = encoder_I.predict(X_test_I)\n",
    "\n",
    "    pd.DataFrame(X_train_encode_I).to_csv(\"../1_Clasifications_models/data_reduced/\" + dependencia_I + \"/DAE/X_train_Norm_DAE_\" + folders_I[index_data_I] + \".csv\", index=False)\n",
    "    pd.DataFrame(X_test_encode_I).to_csv(\"../1_Clasifications_models/data_reduced/\" + dependencia_I + \"/DAE/X_test_Norm_DAE_\" + folders_I[index_data_I] + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
