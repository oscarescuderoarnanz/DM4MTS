{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy as sc\n",
    "\n",
    "import sklearn\n",
    "from sklearn import feature_selection, datasets, model_selection, preprocessing, decomposition, metrics\n",
    "from sklearn.model_selection import validation_curve, learning_curve, cross_validate, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from python_utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../Libraries_Paper/libraries/Osc_libraries')\n",
    "import utils\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I check that the data arrays meet kernel condition...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.io as sio\n",
    "\n",
    "def normData (X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "def normData_minmax (X_train, X_test):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def is_pos_def(A):\n",
    "    M = np.matrix(A)\n",
    "    return np.all(np.linalg.eigvals(M+M.transpose()) > 0)\n",
    "\n",
    "def matrixIsKernel(X_train, X_test):\n",
    "    print(\"Without normalizing the kernel. Data entered to TCK already normalized. \\n Out meets the kernel condition?: \")\n",
    "    print(is_pos_def(X_train))\n",
    "\n",
    "    print(\"Normalizing the kernel. Data entered to TCK already normalized. \\n Out meets the kernel condition?: \")\n",
    "    X_train, X_test = normData(X_train, X_test)\n",
    "    print(is_pos_def(X_train))\n",
    "    \n",
    "    print(\"Normalizing the kernel. Data entered to TCK already normalized. \\n Out meets the kernel condition?: \")\n",
    "    X_train, X_test = normData_minmax(X_train, X_test)\n",
    "    print(is_pos_def(X_train))\n",
    "\n",
    "    \n",
    "def Find_Optimal_Cutoff(target, predicted):\n",
    "    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n",
    "    Parameters\n",
    "    ----------\n",
    "    target : Matrix with dependent or target data, where rows are observations\n",
    "\n",
    "    predicted : Matrix with predicted data, where rows are observations\n",
    "\n",
    "    Returns\n",
    "    -------     \n",
    "    list type, with optimal cutoff value\n",
    "\n",
    "    \"\"\"\n",
    "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "    i = np.arange(len(tpr)) \n",
    "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.loc[(roc.tf-0).abs().argsort()[:1]]\n",
    "\n",
    "    return list(roc_t['threshold']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "def nuSVM(params, folders, dependencia, sigma, random_state, debug):\n",
    "\n",
    "    matrix_all_values = np.zeros((16, len(folders)))\n",
    "\n",
    "    for i in range(len(folders)):\n",
    "        \n",
    "        print()\n",
    "        print(\"###################### RESULTS [SVM] FOLDER \" + folders[i] + \" ######################\")\n",
    "        print()\n",
    "\n",
    "        X_pre_train = pd.read_csv(\"./data_exponential/\" + dependencia + \"/exponential/exponential_kernel_train_\" + folders[i] + \"_VSigma_\" + str(sigma) + \".csv\")\n",
    "        X_test = pd.read_csv(\"./data_exponential/\" + dependencia + \"/exponential/exponential_kernel_test_\" + folders[i] + \"_VSigma_\" + str(sigma) + \".csv\")\n",
    "\n",
    "        y_pre_train = pd.read_csv('../../df_to_load/DataToPaperAndTFM_Mod1/Subconjuntos_3D/' + folders[i] + '/y_train_tensor.csv')\n",
    "        y_test = pd.read_csv('../../df_to_load/DataToPaperAndTFM_Mod1/Subconjuntos_3D/' + folders[i] + '/y_test_tensor.csv')\n",
    "\n",
    "        semillas = [9,18,35,52,75]\n",
    "\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=False)\n",
    "        kf.get_n_splits(X_pre_train)\n",
    "\n",
    "        bestHyperparameters = {'nu': 0}\n",
    "\n",
    "        bestMetricDev = 0\n",
    "        for c in range(len(params['nu'])):\n",
    "            clf = svm.NuSVC(kernel=\"precomputed\", nu=params['nu'][c], probability=True, random_state=random_state)\n",
    "\n",
    "            roc_auc_score = []\n",
    "            threshold_1 = []\n",
    "            threshold_2 = 0\n",
    "            y_pred_arr = []\n",
    "            # Divide the test set into k partitions\n",
    "            for train_index, val_index in kf.split(X_pre_train):\n",
    "\n",
    "                X_train, X_val = X_pre_train.iloc[train_index,train_index].reset_index(drop=True), X_pre_train.iloc[val_index,train_index].reset_index(drop=True)\n",
    "                y_train, y_val = y_pre_train.iloc[train_index].reset_index(drop=True), y_pre_train.iloc[val_index].reset_index(drop=True)\n",
    "\n",
    "                clf = clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict_proba(X_val)[:,1]\n",
    "\n",
    "                threshold_1.append(Find_Optimal_Cutoff(y_val, y_pred))\n",
    "                auc = sklearn.metrics.roc_auc_score(y_val, y_pred)\n",
    "                roc_auc_score.append(auc)\n",
    "\n",
    "                y_pred_arr.append(y_pred)\n",
    "\n",
    "\n",
    "            # Get the threshold for: model configured with hyperparameters with train data\n",
    "            y_pred_ttl = list(y_pred_arr[0])\n",
    "            y_pred_ttl.extend(y_pred_arr[1])\n",
    "            y_pred_ttl.extend(y_pred_arr[2])\n",
    "            y_pred_ttl.extend(y_pred_arr[3])\n",
    "            y_pred_ttl.extend(y_pred_arr[4])\n",
    "\n",
    "            threshold_2 = Find_Optimal_Cutoff(y_pre_train, y_pred_ttl)\n",
    "\n",
    "            if np.mean(roc_auc_score) > bestMetricDev:\n",
    "                print(\"\\Change the best roc auc score \", bestMetricDev, \" by: \", np.mean(roc_auc_score))\n",
    "                bestMetricDev = np.mean(roc_auc_score)\n",
    "                bestHyperparameters['nu'] = params['nu'][c]\n",
    "                bestHyperparameters['threshold_1'] = np.mean(threshold_1)\n",
    "                bestHyperparameters['threshold_2'] = threshold_2\n",
    "                bestHyperparameters['y_pred_val'] = y_pred_ttl\n",
    "\n",
    "        if debug:\n",
    "            print(\"nu: \", bestHyperparameters[\"nu\"])\n",
    "            if (bestHyperparameters[\"nu\"] == np.min(params['nu'])) or (bestHyperparameters[\"nu\"] == np.max(params['nu'])):\n",
    "                print(\"==============================================================\")\n",
    "                print(\"WARNING!!!!!!!\")\n",
    "                print(\"==============================================================\")\n",
    "\n",
    "\n",
    "        clf = svm.NuSVC(kernel=\"precomputed\", nu=bestHyperparameters['nu'], probability=True, random_state=random_state)\n",
    "        clf = clf.fit(X_pre_train, y_pre_train)\n",
    "        y_pred_test = clf.predict_proba(X_test)[:,1]\n",
    "        y_pred_Xtrain = clf.predict(X_pre_train)\n",
    "        \n",
    "        selecCalculateMetrics_aux = ['umbral: 0.5' , \n",
    "                                     'umbral: threshold_2 (datos train)']\n",
    "                        \n",
    "            \n",
    "        for j in range(len(selecCalculateMetrics_aux)):\n",
    "            \n",
    "            v_accuracy_test = []\n",
    "            v_accuracy_train = []\n",
    "            v_specificity = []\n",
    "            v_sensitivity = []\n",
    "            v_precision = []\n",
    "            v_recall = []\n",
    "            v_f1score = []\n",
    "            v_accuracy = []\n",
    "            auc_score = []\n",
    "\n",
    "            if selecCalculateMetrics_aux[j] == 'umbral: 0.5':           \n",
    "                auc_score.append(sklearn.metrics.roc_auc_score(y_test, y_pred_test))\n",
    "                y_pred = (y_pred_test > 0.5).astype('int')\n",
    "            elif selecCalculateMetrics_aux[j] == 'umbral: threshold_2 (datos train)':\n",
    "                auc_score.append(sklearn.metrics.roc_auc_score(y_test, y_pred_test))\n",
    "                y_pred = (y_pred_test > bestHyperparameters[\"threshold_2\"]).astype('int')\n",
    "\n",
    "\n",
    "            v_specificity, v_recall, v_f1score, v_precision, v_accuracy, v_accuracy_test, v_accuracy_train = utils.calculateconfusionmatrix(y_pred, y_pre_train, y_test, v_specificity, v_recall, v_f1score, v_precision, v_accuracy, v_accuracy_test, v_accuracy_train, 0, y_pred_Xtrain)\n",
    "            \n",
    "            matrix_all_values[j*8:j*8 + 8, i] = v_specificity[0], v_recall[0], v_f1score[0], v_precision[0], v_accuracy[0], v_accuracy_test[0], v_accuracy_train[0], auc_score[0]\n",
    "                                 \n",
    "    \n",
    "    print()\n",
    "    print(\"###################### FINAL RESULTS  - With threshold train ######################\")\n",
    "    print()\n",
    "    \n",
    "    utils.printOutAlgorithm(matrix_all_values[8,:], matrix_all_values[9,:], matrix_all_values[10,:], matrix_all_values[11,:], matrix_all_values[12,:], matrix_all_values[13,:], matrix_all_values[14,:], matrix_all_values[15,:])\n",
    "    \n",
    "    return matrix_all_values\n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def LR(params, folders, dependencia, sigma, kfold=5):  \n",
    "    matrix_all_values = np.zeros((16, len(folders)))\n",
    "\n",
    "    for i in range(len(folders)):\n",
    "        \n",
    "        print()\n",
    "        print(\"###################### RESULTS [LR] FOLDER \" + folders[i] + \" ######################\")\n",
    "        print()\n",
    "        \n",
    "        X_pre_train = pd.read_csv(\"./data_exponential/\" + dependencia + \"/exponential/exponential_kernel_train_\" + folders[i] + \"_VSigma_\" + str(sigma) + \".csv\")\n",
    "        X_test = pd.read_csv(\"./data_exponential/\" + dependencia + \"/exponential/exponential_kernel_test_\" + folders[i] + \"_VSigma_\" + str(sigma) + \".csv\")\n",
    "\n",
    "        y_pre_train = pd.read_csv('../../df_to_load/DataToPaperAndTFM_Mod1/Subconjuntos_3D/' + folders[i] + '/y_train_tensor.csv')\n",
    "        y_test = pd.read_csv('../../df_to_load/DataToPaperAndTFM_Mod1/Subconjuntos_3D/' + folders[i] + '/y_test_tensor.csv')\n",
    "\n",
    "        kf = KFold(n_splits=kfold, shuffle=False)\n",
    "        kf.get_n_splits(X_pre_train)\n",
    "\n",
    "        bestHyperparameters = {'C': 0}\n",
    "        \n",
    "        print(\"Dimensions: \")\n",
    "        print(\"X_pre_train:\", X_pre_train.shape)\n",
    "        print(\"X_test:\", X_test.shape)\n",
    "        print(\"y_pre_train:\", y_pre_train.shape)\n",
    "        print(\"y_test:\", y_test.shape)\n",
    "        \n",
    "\n",
    "        bestMetricDev = 0\n",
    "        for c in range(len(params['C'])):\n",
    "            # Build the model with the corresponding hyperparameters\n",
    "            clf = LogisticRegression(solver='liblinear', C=params['C'][c], penalty='l2', n_jobs=32)\n",
    "\n",
    "            roc_auc_score = []\n",
    "            threshold_1 = []\n",
    "            threshold_2 = 0\n",
    "            y_pred_arr = []\n",
    "            # Divide the test set in K partitions\n",
    "            for train_index, val_index in kf.split(X_pre_train):\n",
    "                X_train, X_val = X_pre_train.iloc[train_index].reset_index(drop=True), X_pre_train.iloc[val_index].reset_index(drop=True)\n",
    "                y_train, y_val = y_pre_train.iloc[train_index].reset_index(drop=True), y_pre_train.iloc[val_index].reset_index(drop=True)\n",
    "\n",
    "\n",
    "                clf = clf.fit(np.array(X_train), np.array(y_train))\n",
    "                y_pred = clf.predict_proba(X_val)[:,1]\n",
    "\n",
    "                threshold_1.append(Find_Optimal_Cutoff(y_val, y_pred))\n",
    "                auc = sklearn.metrics.roc_auc_score(y_val, y_pred)\n",
    "                roc_auc_score.append(auc)\n",
    "\n",
    "                y_pred_arr.append(y_pred)\n",
    "\n",
    "            # Get the threshold for: model configured with hyperparameters with train data\n",
    "            y_pred_ttl = list(y_pred_arr[0])\n",
    "            y_pred_ttl.extend(y_pred_arr[1])\n",
    "            y_pred_ttl.extend(y_pred_arr[2])\n",
    "            y_pred_ttl.extend(y_pred_arr[3])\n",
    "            y_pred_ttl.extend(y_pred_arr[4])\n",
    "\n",
    "            threshold_2 = Find_Optimal_Cutoff(y_pre_train, y_pred_ttl)\n",
    "\n",
    "            if np.mean(roc_auc_score) > bestMetricDev:\n",
    "                print(\"\\tChange the best roc auc score \", bestMetricDev, \" by: \", np.mean(roc_auc_score))\n",
    "                bestMetricDev = np.mean(roc_auc_score)\n",
    "                bestHyperparameters['C'] = params['C'][c]\n",
    "                bestHyperparameters['threshold_1'] = np.mean(threshold_1)\n",
    "                bestHyperparameters['threshold_2'] = threshold_2\n",
    "                bestHyperparameters['y_pred_val'] = y_pred_ttl\n",
    "\n",
    "        \n",
    "        print(\"Best roc auc score: \", bestMetricDev)\n",
    "        print(\"C: \", bestHyperparameters[\"C\"])\n",
    "        print(\"threshold obatined with train: \", bestHyperparameters[\"threshold_2\"])\n",
    "        if (bestHyperparameters[\"C\"] == params['C'][0]) or (bestHyperparameters[\"C\"] == params['C'][-1]):\n",
    "            print(\"====================================================================================\")\n",
    "            print(\"Caution!\")\n",
    "            print(\"Value of C at lower or upper extrema\")\n",
    "            print(\"====================================================================================\")\n",
    "        \n",
    "\n",
    "        clf = LogisticRegression(solver='liblinear', C=bestHyperparameters['C'],  penalty='l2', n_jobs=24)\n",
    "        clf = clf.fit(np.array(X_pre_train), np.array(y_pre_train))  \n",
    "        y_pred_test = clf.predict_proba(X_test)[:,1]\n",
    "        y_pred_Xtrain = clf.predict(X_pre_train)\n",
    "        \n",
    "        selecCalculateMetrics_aux = ['umbral: 0.5' , \n",
    "                                     'umbral: threshold_2 (datos train)']\n",
    "                        \n",
    "            \n",
    "        for j in range(len(selecCalculateMetrics_aux)):\n",
    "            \n",
    "            v_accuracy_test = []\n",
    "            v_accuracy_train = []\n",
    "            v_specificity = []\n",
    "            v_sensitivity = []\n",
    "            v_precision = []\n",
    "            v_recall = []\n",
    "            v_f1score = []\n",
    "            v_accuracy = []\n",
    "            auc_score = []\n",
    "\n",
    "            print(\"folder: \" + str(folders[i]) + \" | y_test.shape: \" + str(y_test.shape)+ \" | y_pred_test.shape: \" + str(y_pred_test.shape))\n",
    "            \n",
    "            if selecCalculateMetrics_aux[j] == 'umbral: 0.5':\n",
    "                auc_score.append(sklearn.metrics.roc_auc_score(y_test, y_pred_test))\n",
    "                y_pred = (y_pred_test > 0.5).astype('int')\n",
    "            elif selecCalculateMetrics_aux[j] == 'umbral: threshold_2 (datos train)':\n",
    "                auc_score.append(sklearn.metrics.roc_auc_score(y_test, y_pred_test))\n",
    "                y_pred = (y_pred_test > bestHyperparameters[\"threshold_2\"]).astype('int')\n",
    "\n",
    "\n",
    "            v_specificity, v_recall, v_f1score, v_precision, v_accuracy, v_accuracy_test, v_accuracy_train = utils.calculateconfusionmatrix(y_pred, y_pre_train, y_test, v_specificity, v_recall, v_f1score, v_precision, v_accuracy, v_accuracy_test, v_accuracy_train, 0, y_pred_Xtrain)\n",
    "            \n",
    "            matrix_all_values[j*8:j*8 + 8, i] = v_specificity[0], v_recall[0], v_f1score[0], v_precision[0], v_accuracy[0], v_accuracy_test[0], v_accuracy_train[0], auc_score[0]\n",
    "                                 \n",
    "                \n",
    "    print()\n",
    "    print(\"###################### FINAL RESULTS - With threshold train ######################\")\n",
    "    print()\n",
    "    \n",
    "    utils.printOutAlgorithm(matrix_all_values[8,:], matrix_all_values[9,:], matrix_all_values[10,:], matrix_all_values[11,:], matrix_all_values[12,:], matrix_all_values[13,:], matrix_all_values[14,:], matrix_all_values[15,:])\n",
    "    \n",
    "    return matrix_all_values\n",
    "    \n",
    "    \n",
    "        \n",
    "def randomForest(params, folders, dependencia, sigma, kfold=5):  \n",
    "    matrix_all_values = np.zeros((16, len(folders)))\n",
    "\n",
    "    for i in range(len(folders)):\n",
    "        \n",
    "        print()\n",
    "        print(\"###################### RESULTS [random forest] FOLDER \" + folders[i] + \" ######################\")\n",
    "        print()\n",
    "        \n",
    "        X_pre_train = pd.read_csv(\"./data_exponential/\" + dependencia + \"/exponential/exponential_kernel_train_\" + folders[i] + \"_VSigma_\" + str(sigma) + \".csv\")\n",
    "        X_test = pd.read_csv(\"./data_exponential/\" + dependencia + \"/exponential/exponential_kernel_test_\" + folders[i] + \"_VSigma_\" + str(sigma) + \".csv\")\n",
    "\n",
    "        y_pre_train = pd.read_csv('../../df_to_load/DataToPaperAndTFM_Mod1/Subconjuntos_3D/' + folders[i] + '/y_train_tensor.csv')\n",
    "        y_test = pd.read_csv('../../df_to_load/DataToPaperAndTFM_Mod1/Subconjuntos_3D/' + folders[i] + '/y_test_tensor.csv')\n",
    "\n",
    "        kf = KFold(n_splits=kfold, shuffle=False)\n",
    "        kf.get_n_splits(X_pre_train)\n",
    "\n",
    "        bestHyperparameters = {'min_samples_leaf': 0, 'max_depth':0, 'n_estimators':0}\n",
    "\n",
    "        bestMetricDev = 0\n",
    "        for msl in range(len(params['min_samples_leaf'])):\n",
    "            for md in range(len(params['max_depth'])):\n",
    "                for ne in range(len(params['n_estimators'])):\n",
    "                    clf = RandomForestClassifier(min_samples_leaf=params['min_samples_leaf'][msl], max_depth=params['max_depth'][md], n_estimators=params['n_estimators'][ne], n_jobs=48)\n",
    "\n",
    "                    roc_auc_score = []\n",
    "                    threshold_1 = []\n",
    "                    threshold_2 = 0\n",
    "                    y_pred_arr = []\n",
    "                    # Divide the test set in K partitions\n",
    "                    for train_index, val_index in kf.split(X_pre_train):\n",
    "                        X_train, X_val = X_pre_train.iloc[train_index].reset_index(drop=True), X_pre_train.iloc[val_index].reset_index(drop=True)\n",
    "                        y_train, y_val = y_pre_train.iloc[train_index].reset_index(drop=True), y_pre_train.iloc[val_index].reset_index(drop=True)\n",
    "\n",
    "\n",
    "                        clf = clf.fit(np.array(X_train), np.array(y_train))\n",
    "                        y_pred = clf.predict_proba(X_val)[:,1]\n",
    "\n",
    "                        threshold_1.append(Find_Optimal_Cutoff(y_val, y_pred))\n",
    "                        auc = sklearn.metrics.roc_auc_score(y_val, y_pred)\n",
    "                        roc_auc_score.append(auc)\n",
    "\n",
    "                        y_pred_arr.append(y_pred)\n",
    "\n",
    "                    # Get the threshold for: model configured with hyperparameters with train data\n",
    "                    y_pred_ttl = list(y_pred_arr[0])\n",
    "                    y_pred_ttl.extend(y_pred_arr[1])\n",
    "                    y_pred_ttl.extend(y_pred_arr[2])\n",
    "                    y_pred_ttl.extend(y_pred_arr[3])\n",
    "                    y_pred_ttl.extend(y_pred_arr[4])\n",
    "\n",
    "                    threshold_2 = Find_Optimal_Cutoff(y_pre_train, y_pred_ttl)\n",
    "                    \n",
    "                    if np.mean(roc_auc_score) > bestMetricDev:\n",
    "                        #print(\"\\tChange the best roc auc score \", bestMetricDev, \" by: \", np.mean(roc_auc_score))\n",
    "                        bestMetricDev = np.mean(roc_auc_score)\n",
    "                        bestHyperparameters['min_samples_leaf'] = params['min_samples_leaf'][msl]\n",
    "                        bestHyperparameters['max_depth'] = params['max_depth'][md]\n",
    "                        bestHyperparameters['n_estimators'] = params['n_estimators'][ne]\n",
    "                        bestHyperparameters['threshold_1'] = np.mean(threshold_1)\n",
    "                        bestHyperparameters['threshold_2'] = threshold_2\n",
    "                        bestHyperparameters['y_pred_val'] = y_pred_ttl\n",
    "\n",
    "\n",
    "        print(\"Best roc auc score: \", bestMetricDev)\n",
    "        print(\"min_samples_leaf: \", bestHyperparameters[\"min_samples_leaf\"])\n",
    "        print(\"max_depth: \", bestHyperparameters[\"max_depth\"])\n",
    "        print(\"n_estimators: \", bestHyperparameters[\"n_estimators\"])\n",
    "        print(\"threshold obtained with train: \", bestHyperparameters[\"threshold_2\"])\n",
    "\n",
    "        if (bestHyperparameters[\"min_samples_leaf\"] == params['min_samples_leaf'][0]) or (bestHyperparameters[\"min_samples_leaf\"] == params['min_samples_leaf'][-1]):\n",
    "            print(\"====================================================================================\")\n",
    "            print(\"Caution!\")\n",
    "            print(\"Value of min_samples_leaf at low end or high extrema\")\n",
    "            print(\"====================================================================================\")\n",
    "        if (bestHyperparameters[\"max_depth\"] == params['max_depth'][0]) or (bestHyperparameters[\"max_depth\"] == params['max_depth'][-1]):\n",
    "            print(\"====================================================================================\")\n",
    "            print(\"Caution!\")\n",
    "            print(\"Value of max_depth at low end or high extrema\")\n",
    "            print(\"====================================================================================\")\n",
    "        if (bestHyperparameters[\"n_estimators\"] == params['n_estimators'][0]) or (bestHyperparameters[\"n_estimators\"] == params['n_estimators'][-1]):\n",
    "            print(\"====================================================================================\")\n",
    "            print(\"Caution!\")\n",
    "            print(\"Value of n_estimators at low end or high extrema\")\n",
    "            print(\"====================================================================================\")\n",
    "\n",
    "\n",
    "        clf = RandomForestClassifier(min_samples_leaf=bestHyperparameters['min_samples_leaf'], max_depth=bestHyperparameters['max_depth'], n_estimators=bestHyperparameters['n_estimators'], n_jobs=48)\n",
    "        clf = clf.fit(np.array(X_pre_train), np.array(y_pre_train))  \n",
    "        y_pred_test = clf.predict_proba(X_test)[:,1]\n",
    "        y_pred_Xtrain = clf.predict(X_pre_train)\n",
    "        \n",
    "\n",
    "        y_pred_Xtrain = clf.predict(X_pre_train)\n",
    "        \n",
    "        selecCalculateMetrics_aux = ['umbral: 0.5' , \n",
    "                                     'umbral: threshold_2 (datos train)']                        \n",
    "            \n",
    "        for j in range(len(selecCalculateMetrics_aux)):\n",
    "            \n",
    "            v_accuracy_test = []\n",
    "            v_accuracy_train = []\n",
    "            v_specificity = []\n",
    "            v_sensitivity = []\n",
    "            v_precision = []\n",
    "            v_recall = []\n",
    "            v_f1score = []\n",
    "            v_accuracy = []\n",
    "            auc_score = []\n",
    "\n",
    "            if selecCalculateMetrics_aux[j] == 'umbral: 0.5':\n",
    "                auc_score.append(sklearn.metrics.roc_auc_score(y_test, y_pred_test))\n",
    "                y_pred = (y_pred_test > 0.5).astype('int')\n",
    "            elif selecCalculateMetrics_aux[j] == 'umbral: threshold_2 (datos train)':\n",
    "                auc_score.append(sklearn.metrics.roc_auc_score(y_test, y_pred_test))\n",
    "                y_pred = (y_pred_test > bestHyperparameters[\"threshold_2\"]).astype('int')\n",
    "\n",
    "            v_specificity, v_recall, v_f1score, v_precision, v_accuracy, v_accuracy_test, v_accuracy_train = utils.calculateconfusionmatrix(y_pred, y_pre_train, y_test, v_specificity, v_recall, v_f1score, v_precision, v_accuracy, v_accuracy_test, v_accuracy_train, 0, y_pred_Xtrain)\n",
    "            \n",
    "            matrix_all_values[j*8:j*8 + 8, i] = v_specificity[0], v_recall[0], v_f1score[0], v_precision[0], v_accuracy[0], v_accuracy_test[0], v_accuracy_train[0], auc_score[0]\n",
    "                                 \n",
    "        \n",
    "    print()\n",
    "    print(\"###################### FINAL RESULTS RF ######################\")\n",
    "    print()\n",
    "    \n",
    "    print(\"====> WITH THRESHOLD OBTAINED WITH THE TRAIN DATA\")\n",
    "    print()\n",
    "    utils.printOutAlgorithm(matrix_all_values[8,:], matrix_all_values[9,:], matrix_all_values[10,:], matrix_all_values[11,:], matrix_all_values[12,:], matrix_all_values[13,:], matrix_all_values[14,:], matrix_all_values[15,:])\n",
    "\n",
    "       \n",
    "    return matrix_all_values\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = \"none\"\n",
    "folders = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "sigma = [0.8, 0.85, 0.9, 0.95, 1, 1.25, 1.5]\n",
    "dependencia=\"D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index_sigma in range(len(sigma)):\n",
    "    for index_data in range(len(folders)):\n",
    "        X_pre_train = pd.read_csv('../../Step1_DTW/data_generated_by_dtw/DTW_D/'+ folders[index_data] + '/X_train.csv')\n",
    "        X_test = pd.read_csv('../../Step1_DTW/data_generated_by_dtw/DTW_D/'+ folders[index_data] + '/X_test.csv')\n",
    "        print(X_pre_train.shape)\n",
    "        print(X_test.shape)\n",
    "        \n",
    "        if norm == \"sqrt_max\":\n",
    "            print(\"Normalization sqrt max\")\n",
    "            X_train_aux = np.zeros((X_pre_train.shape[0],X_pre_train.shape[1]))\n",
    "            for index in range(X_pre_train.shape[0]): \n",
    "                X_train_aux[:,index] = X_pre_train.iloc[:,index] / np.sqrt(np.max(X_pre_train.iloc[:,index]))\n",
    "                X_train_aux[index,:] = X_pre_train.iloc[index,:] / np.sqrt(np.max(X_pre_train.iloc[:,index]))\n",
    "            X_pre_train = pd.DataFrame(X_train_aux)\n",
    "\n",
    "            X_test_aux = np.zeros((X_test.shape[0],X_test.shape[1]))\n",
    "            for index in range(X_test.shape[1]): \n",
    "                X_test_aux[:,index] = X_test.iloc[:,index] / np.sqrt(np.max(X_test.iloc[:,index]))\n",
    "                X_test_aux[index,:] = X_test.iloc[index,:] / np.sqrt(np.max(X_test.iloc[index,:]))\n",
    "            X_test = pd.DataFrame(X_test_aux)  \n",
    "        \n",
    "        else:\n",
    "            print(\"No normalization...\")\n",
    "\n",
    "        exponential_dtw_kernel = pd.DataFrame(np.exp(-X_pre_train/(2*sigma[index_sigma]**2)))\n",
    "        exponential_dtw_kernel_test = pd.DataFrame(np.exp(-X_test/(2*sigma[index_sigma]**2)))\n",
    "\n",
    "        exponential_dtw_kernel = np.round(exponential_dtw_kernel, 6)\n",
    "        exponential_dtw_kernel_test = np.round(exponential_dtw_kernel_test, 6)\n",
    "\n",
    "        print(exponential_dtw_kernel.loc[0])\n",
    "\n",
    "        eigenvalues = numpy.linalg.eig(exponential_dtw_kernel)\n",
    "        matriz = np.array(exponential_dtw_kernel)\n",
    "        if (np.sum(np.abs(eigenvalues[0])>0) == exponential_dtw_kernel.shape[0]) and (np.array_equal(matriz, matriz.T)):\n",
    "            print(\"---------------------------\")\n",
    "            print(\"Positive eigenvalue and symmetric matrix\")\n",
    "            print(\"---------------------------\")\n",
    "            print(\"Meets kernel condition\")\n",
    "            print(\"---------------------------\")\n",
    "        else:\n",
    "            print(\"==============================\")\n",
    "            print(\"NO Meets kernel condition\")\n",
    "            print(\"==============================\")\n",
    "            \n",
    "        exponential_dtw_kernel.to_csv(\"./data_exponential/D/exponential/exponential_kernel_train_\" + folders[index_data] + \"_VSigma_\" + str(sigma[index_sigma]) + \".csv\", index=False)\n",
    "        exponential_dtw_kernel_test.to_csv(\"./data_exponential/D/exponential/exponential_kernel_test_\" + folders[index_data] + \"_VSigma_\" + str(sigma[index_sigma]) + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ttl_05_DTW_D = {}\n",
    "results_ttl_train_DTW_D = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LR\n",
    "C = [.0000001, .000001, .00001, .0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, .75, 1, 3, 5, 8, 10 ,12, 15]\n",
    "params = {'C': C}\n",
    "debug = True\n",
    "bestROC = 0\n",
    "best_value_sigma = 0\n",
    "for index in range(len(sigma)):\n",
    "    print(\"================================================================================================\")\n",
    "    print(sigma[index])\n",
    "    print(\"================================================================================================\")\n",
    "        \n",
    "    results = LR(params, folders, dependencia, sigma[index])\n",
    "    if bestROC < np.mean(results[7, :]):\n",
    "        bestROC = np.mean(results[7, :])\n",
    "        best_value_sigma = sigma[index]\n",
    "\n",
    "print()\n",
    "print(\"Best roc: \", bestROC, \" - value SIGMA: \", best_value_sigma)   \n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"BEST COMBINATION OF PARAMETERS\")\n",
    "print(\"===============================================\")\n",
    "results_LR_DTW_D = LR(params, folders, dependencia, best_value_sigma)\n",
    "results_ttl_05_DTW_D['LR'] = results_LR_DTW_D[[0,1,7], :]\n",
    "results_ttl_train_DTW_D['LR'] = results_LR_DTW_D[[8,9,15], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RF\n",
    "folders = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "\n",
    "norm = \"none\"\n",
    "max_depth = np.arange(10, 42, 4)\n",
    "min_samples_leaf = np.array([2, 4, 9,  13,  17,  22,  43])\n",
    "n_estimators = np.array([30, 50, 100, 200, 400, 600])\n",
    "\n",
    "params = {'max_depth': max_depth,\n",
    "         'min_samples_leaf': min_samples_leaf,\n",
    "         'n_estimators': n_estimators}\n",
    "\n",
    "bestROC = 0\n",
    "best_value_sigma = 0\n",
    "for index in range(len(sigma)):\n",
    "    print(\"================================================================================================\")\n",
    "    print(sigma[index])\n",
    "    print(\"================================================================================================\")\n",
    "        \n",
    "    results = randomForest(params, folders, dependencia, sigma[index])\n",
    "    if bestROC < np.mean(results[7, :]):\n",
    "        bestROC = np.mean(results[7, :])\n",
    "        best_value_sigma = sigma[index]\n",
    "\n",
    "print()\n",
    "print(\"Best roc: \", bestROC, \" - value SIGMA: \", best_value_sigma)   \n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"BEST COMBINATION OF PARAMETERS\")\n",
    "print(\"===============================================\")\n",
    "results_RF_DTW_D = randomForest(params, folders, dependencia, best_value_sigma)\n",
    "results_ttl_05_DTW_D['RF'] = results_RF_DTW_D[[0,1,7], :]\n",
    "results_ttl_train_DTW_D['RF'] = results_RF_DTW_D[[8,9,15], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuSVM\n",
    "folders = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "\n",
    "norm = \"none\"\n",
    "nu = [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 0.75, 0.9]\n",
    "params = {'nu': nu}\n",
    "debug = True\n",
    "random_state = 30\n",
    "\n",
    "bestROC = 0\n",
    "best_value_sigma = 0\n",
    "for index in range(len(sigma)):\n",
    "    print(\"================================================================================================\")\n",
    "    print(sigma[index])\n",
    "    print(\"================================================================================================\")\n",
    "\n",
    "    results = nuSVM(params, folders, dependencia, sigma[index], random_state, debug)\n",
    "    if bestROC < np.mean(results[7, :]):\n",
    "        bestROC = np.mean(results[7, :])\n",
    "        best_value_sigma = sigma[index]\n",
    "\n",
    "print()\n",
    "print(\"Best roc: \", bestROC, \" - value gamma: \", best_value_sigma)   \n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"BEST COMBINATION OF PARAMETERS\")\n",
    "print(\"===============================================\")\n",
    "results_nuSVM_DTW_D = nuSVM(params, folders, dependencia, best_value_sigma, random_state, debug)\n",
    "results_ttl_05_DTW_D['nuSVM'] = results_nuSVM_DTW_D[[0,1,7], :]\n",
    "results_ttl_train_DTW_D['nuSVM'] = results_nuSVM_DTW_D[[8,9,15], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = \"none\"\n",
    "folders = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "sigma = [5.5, 6.5, 7.5, 8.5, 9.5, 10.5]\n",
    "dependencia=\"I\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_sigma in range(len(sigma)):\n",
    "    for index_data in range(len(folders)):\n",
    "        \n",
    "        X_pre_train = pd.read_csv('../../Step1_DTW/data_generated_by_dtw/DTW_I/'+ folders[index_data] + '/X_train.csv')\n",
    "        X_test = pd.read_csv('../../Step1_DTW/data_generated_by_dtw/DTW_I/'+ folders[index_data] + '/X_test.csv')\n",
    "        print(X_pre_train.shape)\n",
    "        print(X_test.shape)\n",
    "        \n",
    "        if norm == \"sqrt_max\":\n",
    "            print(\"Normalization sqrt max\")\n",
    "            X_train_aux = np.zeros((X_pre_train.shape[0],X_pre_train.shape[1]))\n",
    "            for index in range(X_pre_train.shape[0]): \n",
    "                X_train_aux[:,index] = X_pre_train.iloc[:,index] / np.sqrt(np.max(X_pre_train.iloc[:,index]))\n",
    "                X_train_aux[index,:] = X_pre_train.iloc[index,:] / np.sqrt(np.max(X_pre_train.iloc[:,index]))\n",
    "            X_pre_train = pd.DataFrame(X_train_aux)\n",
    "\n",
    "            X_test_aux = np.zeros((X_test.shape[0],X_test.shape[1]))\n",
    "            for index in range(X_test.shape[1]): \n",
    "                X_test_aux[:,index] = X_test.iloc[:,index] / np.sqrt(np.max(X_test.iloc[:,index]))\n",
    "                X_test_aux[index,:] = X_test.iloc[index,:] / np.sqrt(np.max(X_test.iloc[index,:]))\n",
    "            X_test = pd.DataFrame(X_test_aux)  \n",
    "        \n",
    "        else:\n",
    "            print(\"No normalization...\")\n",
    "\n",
    "        exponential_dtw_kernel = pd.DataFrame(np.exp(-X_pre_train/(2*sigma[index_sigma]**2)))\n",
    "        exponential_dtw_kernel_test = pd.DataFrame(np.exp(-X_test/(2*sigma[index_sigma]**2)))\n",
    "\n",
    "        exponential_dtw_kernel = np.round(exponential_dtw_kernel, 6)\n",
    "        exponential_dtw_kernel_test = np.round(exponential_dtw_kernel_test, 6)\n",
    "\n",
    "        print(exponential_dtw_kernel.loc[0])\n",
    "\n",
    "        eigenvalues = numpy.linalg.eig(exponential_dtw_kernel)\n",
    "        matriz = np.array(exponential_dtw_kernel)\n",
    "        if (np.sum(np.abs(eigenvalues[0])>0) == exponential_dtw_kernel.shape[0]) and (np.array_equal(matriz, matriz.T)):\n",
    "            print(\"---------------------------\")\n",
    "            print(\"Positive eigenvalue and symmetric matrix\")\n",
    "            print(\"---------------------------\")\n",
    "            print(\"Meets kernel condition\")\n",
    "            print(\"---------------------------\")\n",
    "        else:\n",
    "            print(\"==============================\")\n",
    "            print(\"NO Meets kernel condition\")\n",
    "            print(\"==============================\")\n",
    "            \n",
    "        exponential_dtw_kernel.to_csv(\"./data_exponential/I/exponential/exponential_kernel_train_\" + folders[index_data] + \"_VSigma_\" + str(sigma[index_sigma]) + \".csv\", index=False)\n",
    "        exponential_dtw_kernel_test.to_csv(\"./data_exponential/I/exponential/exponential_kernel_test_\" + folders[index_data] + \"_VSigma_\" + str(sigma[index_sigma]) + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ttl_05_DTW_I = {}\n",
    "results_ttl_train_DTW_I = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LR\n",
    "C = [.0000001, .000001, .00001, .0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, .75, 1, 3, 5, 8, 10 ,12, 15]\n",
    "params = {'C': C}\n",
    "\n",
    "bestROC = 0\n",
    "best_value_sigma = 0\n",
    "for index in range(len(sigma)):\n",
    "    print(\"================================================================================================\")\n",
    "    print(sigma[index])\n",
    "    print(\"================================================================================================\")\n",
    "        \n",
    "    results = LR(params, folders, dependencia, sigma[index])\n",
    "    if bestROC < np.mean(results[7, :]):\n",
    "        bestROC = np.mean(results[7, :])\n",
    "        best_value_sigma = sigma[index]\n",
    "\n",
    "print()\n",
    "print(\"Best roc: \", bestROC, \" - value SIGMA: \", best_value_sigma)   \n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"BEST COMBINATION OF PARAMETERS\")\n",
    "print(\"===============================================\")\n",
    "results_LR_DTW_I = LR(params, folders, dependencia, best_value_sigma)\n",
    "results_ttl_05_DTW_I['LR'] = results_LR_DTW_I[[0,1,7], :]\n",
    "results_ttl_train_DTW_I['LR'] = results_LR_DTW_I[[8,9,15], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RF\n",
    "max_depth = np.arange(10, 42, 4)\n",
    "min_samples_leaf = np.array([2, 4, 9,  13,  17,  22,  43])\n",
    "n_estimators = np.array([30, 50, 100, 200, 400, 600])\n",
    "\n",
    "params = {'max_depth': max_depth,\n",
    "         'min_samples_leaf': min_samples_leaf,\n",
    "         'n_estimators': n_estimators}\n",
    "\n",
    "bestROC = 0\n",
    "best_value_sigma = 0\n",
    "for index in range(len(sigma)):\n",
    "    print(\"================================================================================================\")\n",
    "    print(sigma[index])\n",
    "    print(\"================================================================================================\")\n",
    "        \n",
    "    results = randomForest(params, folders, dependencia, sigma[index])\n",
    "    if bestROC < np.mean(results[7, :]):\n",
    "        bestROC = np.mean(results[7, :])\n",
    "        best_value_sigma = sigma[index]\n",
    "\n",
    "print()\n",
    "print(\"Best roc: \", bestROC, \" - value SIGMA: \", best_value_sigma)   \n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"BEST COMBINATION OF PARAMETERS\")\n",
    "print(\"===============================================\")\n",
    "results_RF_DTW_I = randomForest(params, folders, dependencia, best_value_sigma)\n",
    "results_ttl_05_DTW_I['RF'] = results_RF_DTW_I[[0,1,7], :]\n",
    "results_ttl_train_DTW_I['RF'] = results_RF_DTW_I[[8,9,15], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nuSVM\n",
    "nu = [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 0.75, 0.9]\n",
    "params = {'nu': nu}\n",
    "debug = True\n",
    "random_state = 30\n",
    "\n",
    "\n",
    "bestROC = 0\n",
    "best_value_sigma = 0\n",
    "for index in range(len(sigma)):\n",
    "    print(\"================================================================================================\")\n",
    "    print(sigma[index])\n",
    "    print(\"================================================================================================\")\n",
    "\n",
    "    results = nuSVM(params, folders, dependencia, sigma[index], random_state, debug)\n",
    "    if bestROC < np.mean(results[7, :]):\n",
    "        bestROC = np.mean(results[7, :])\n",
    "        best_value_sigma = sigma[index]\n",
    "\n",
    "print()\n",
    "print(\"Best roc: \", bestROC, \" - value gamma: \", best_value_sigma)   \n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"BEST COMBINATION OF PARAMETERS\")\n",
    "print(\"===============================================\")\n",
    "results_nuSVM_DTW_I = nuSVM(params, folders, dependencia, best_value_sigma, random_state, debug) \n",
    "results_ttl_05_DTW_I['nuSVM'] = results_nuSVM_DTW_I[[0,1,7], :]\n",
    "results_ttl_train_DTW_I['nuSVM'] = results_nuSVM_DTW_I[[8,9,15], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveData(results_ttl, name):\n",
    "    print(list(results_ttl.keys()))\n",
    "    values = list(results_ttl.values())\n",
    "    arr = []\n",
    "    for i in range(len(values)):\n",
    "        for j in range(len(values[i])):\n",
    "            arr.append(values[i][j])\n",
    "\n",
    "    df_results_ttl_AE = pd.DataFrame(arr)\n",
    "    print(df_results_ttl_AE.shape)\n",
    "    # rows: specificity, sensitivity and roc-auc\n",
    "    # columns: folders\n",
    "    df_results_ttl_AE.to_excel(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData(results_ttl_05_DTW_D, '../results_kernels/DTW/D/exponential_05.xlsx')\n",
    "\n",
    "saveData(results_ttl_05_DTW_I, '../results_kernels/DTW/I/exponential_05.xlsx')\n",
    "\n",
    "saveData(results_ttl_train_DTW_D, '../results_kernels/DTW/D/exponential_train.xlsx')\n",
    "\n",
    "saveData(results_ttl_train_DTW_I, '../results_kernels/DTW/I/exponential_train.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
