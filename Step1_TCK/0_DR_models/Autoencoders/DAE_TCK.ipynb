{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import AE_models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras \n",
    "from keras import layers\n",
    "import scipy.io as sio\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_range_analysis(X):\n",
    "    if sum(sum(((np.round(X,4) <= 1)) & (X >= 0))) == X.shape[0] * X.shape[1]:\n",
    "        print(\"Range value: [0,1]\")\n",
    "    else:\n",
    "        print(\"ERROR!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLossesLayers(h_layers, dicc, carpeta):\n",
    "    plot_layers = []\n",
    "    for i in range(len(h_layers)):\n",
    "        plot_layers.append(h_layers[i][1])\n",
    "\n",
    "    keys = list(dicc.keys())\n",
    "    \n",
    "    filas = int(np.ceil(len(keys)/2))\n",
    "    columnas = int(np.sqrt(np.round(len(keys)/2)))\n",
    "    \n",
    "    f, axis = plt.subplots(filas, columnas, figsize=(20,20))\n",
    "    index_keys = 0\n",
    "    for i in range(filas):\n",
    "        for j in range(columnas):\n",
    "            axis[i,j].plot(plot_layers, dicc[keys[index_keys]][0])\n",
    "            axis[i,j].plot(plot_layers, dicc[keys[index_keys]][1])\n",
    "            axis[i,j].legend([\"val_loss\", \"loss\"])\n",
    "            axis[i,j].set_title(keys[i])\n",
    "            index_keys += 1\n",
    "\n",
    "    plt.close(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folders = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "normalizar = True\n",
    "plotAndSaveImg = False\n",
    "\n",
    "for index_data in range(len(folders)):\n",
    "    print(\"================================================================>\" + folders[index_data] + \"<=======================================================\")\n",
    "    X_train_original, X_test_original, y_train_original, y_test_original = AE_models.loadData(index_data, normalizar, folders)\n",
    "    value_range_analysis(X_train_original)\n",
    "\n",
    "    autoencodertype = {'DAE': True, 'AE': False}\n",
    "\n",
    "\n",
    "    epochs = 5000\n",
    "    initial_learning_rate = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    batch_size = [32]\n",
    "    dropout=[0, .05, .1]\n",
    "    std_noise = [0.01, 0.025, 0.05]\n",
    "    h_layers = [[X_train_original.shape[0], 525],\n",
    "                [X_train_original.shape[0], 500],\n",
    "                [X_train_original.shape[0], 475],\n",
    "                [X_train_original.shape[0], 450],\n",
    "                [X_train_original.shape[0], 425],\n",
    "                [X_train_original.shape[0], 400],\n",
    "                [X_train_original.shape[0], 375],\n",
    "                [X_train_original.shape[0],  350],\n",
    "                [X_train_original.shape[0],  325],\n",
    "                [X_train_original.shape[0],  300],\n",
    "                [X_train_original.shape[0], 275]]\n",
    "\n",
    "\n",
    "    hyperparameters = {'epochs': epochs,\n",
    "                       'initial_learning_rate':initial_learning_rate,\n",
    "                       'kfold':5,\n",
    "                       \"mindelta\": 0.0001}\n",
    "\n",
    "    y_pre_train = y_train_original\n",
    "    y_test = y_test_original\n",
    "    X_pre_train = X_train_original\n",
    "    X_test = X_test_original\n",
    "\n",
    "    semillas = [9,18,35, 52, 75]\n",
    "\n",
    "    kf = KFold(n_splits=hyperparameters[\"kfold\"], shuffle=True, random_state=semillas[index_data])\n",
    "    kf.get_n_splits(X_pre_train)\n",
    "\n",
    "    bestHyperparameters = {'h_layers': -1, 'batch_size': -1, 'initial_learning_rate': -1 }\n",
    "\n",
    "    bestMetricDev = np.inf\n",
    "    dicc = {}\n",
    "    for bs in range(len(batch_size)):\n",
    "        for d in range(len(dropout)):\n",
    "            for init in range(len(initial_learning_rate)):\n",
    "                for std in range(len(std_noise)):\n",
    "                    plot_val_loss = []\n",
    "                    plot_loss = []\n",
    "                    for l in range(len(h_layers)):\n",
    "                        hyperparameters_aux = {'epochs': hyperparameters['epochs'],\n",
    "                                           'initial_learning_rate': hyperparameters['initial_learning_rate'][init],\n",
    "                                           \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "                                           'batch_size': batch_size[bs],\n",
    "                                           'h_layers': h_layers[l],\n",
    "                                           'dropout': dropout[d],\n",
    "                                           'std_noise': std_noise[std],\n",
    "                                           'verbose': 0}\n",
    "\n",
    "                        v_early = []\n",
    "                        v_metric_dev = []\n",
    "                        v_hist = []\n",
    "                        v_val_loss = []\n",
    "                        for train_index, val_index in kf.split(X_pre_train):\n",
    "                            X_train, X_val = X_pre_train[train_index], X_pre_train[val_index]\n",
    "                            AE_models.reset_keras()\n",
    "                            model, hist, early = AE_models.runNetwork(X_train, X_val,\n",
    "                                                                      hyperparameters_aux,\n",
    "                                                                      autoencodertype)\n",
    "\n",
    "\n",
    "                            v_early.append(early)\n",
    "                            v_hist.append(hist)\n",
    "                            v_val_loss.append(np.min(hist.history[\"val_loss\"]))\n",
    "                        metric_dev = np.mean(v_val_loss)\n",
    "                        plot_val_loss.append(metric_dev)\n",
    "                        plot_loss.append(np.mean(hist.history[\"loss\"]))\n",
    "\n",
    "                        if metric_dev < bestMetricDev:\n",
    "                            print(\"\\tChange the best \", bestMetricDev, \" by metric dev: \", metric_dev)\n",
    "                            bestMetricDev = metric_dev\n",
    "                            bestHyperparameters['h_layers'] = l\n",
    "                            bestHyperparameters['batch_size'] = bs\n",
    "                            bestHyperparameters['initial_learning_rate'] = init\n",
    "                            bestHyperparameters['std_noise'] = std\n",
    "                            bestHyperparameters['dropout'] = d\n",
    "                            print(\"%%%%%%%%%bestHyperparameters%%%%%%%%%\")\n",
    "                            print(bestHyperparameters)\n",
    "\n",
    "                    dicc[\"bs:\" + str(batch_size[bs]) + \"-lr:\" + str(initial_learning_rate[init]) + \"-std:\" + str(std_noise[std])] = [plot_val_loss, plot_loss]\n",
    "\n",
    "\n",
    "\n",
    "    if plotAndSaveImg:\n",
    "        plotLossesLayers(h_layers, dicc, folders[index_data])\n",
    "\n",
    "    print(\"Layers selected:\", h_layers[bestHyperparameters[\"h_layers\"]])\n",
    "    print(\"batch_size selected:\", batch_size[bestHyperparameters[\"batch_size\"]])\n",
    "    print(\"initial_learning_rate selected:\", initial_learning_rate[bestHyperparameters[\"initial_learning_rate\"]])\n",
    "    print(\"std_noise selected:\", std_noise[bestHyperparameters[\"std_noise\"]])\n",
    "    print(\"std_noise selected:\", dropout[bestHyperparameters[\"dropout\"]])\n",
    "\n",
    "    print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%================================================================> BEST MODEL SELECT WITH DIFFERENTS HYPERPARAMETERS <=======================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "    hyperparameters = {'epochs': hyperparameters['epochs'],\n",
    "                   'initial_learning_rate': initial_learning_rate[bestHyperparameters[\"initial_learning_rate\"]],\n",
    "                   \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "                   'batch_size': batch_size[bestHyperparameters[\"batch_size\"]],\n",
    "                   'h_layers': h_layers[bestHyperparameters[\"h_layers\"]],\n",
    "                   'std_noise': std_noise[bestHyperparameters[\"std_noise\"]],\n",
    "                   'dropout': std_noise[bestHyperparameters[\"dropout\"]],\n",
    "                   'verbose': 1}\n",
    "    \n",
    "    AE_models.reset_keras()\n",
    "    X_train_aux, X_val_aux = train_test_split(X_pre_train,\n",
    "                                              test_size=0.3,\n",
    "                                              random_state=semillas[index_data])\n",
    "    \n",
    "    autoencoder, hist, early = AE_models.runNetwork(X_train_aux, X_val_aux,\n",
    "                                                    hyperparameters,\n",
    "                                                    autoencodertype)\n",
    "\n",
    "\n",
    "\n",
    "    encoder = keras.Model(autoencoder.input, autoencoder.get_layer('Latent_layer').output)\n",
    "\n",
    "    X_train = sio.loadmat('../../data_reduced_by_tck/' + folders[index_data] + '/Ktrtr')\n",
    "    X_test = sio.loadmat('../../data_reduced_by_tck/' + folders[index_data] + '/Ktrte')\n",
    "    X_train = X_train['Ktrtr']\n",
    "    X_test = X_test['Ktrte']\n",
    "    X_test = X_test.T\n",
    "\n",
    "    X_train, X_test = AE_models.normData_minmax(X_train, X_test)\n",
    "\n",
    "    encoder = keras.Model(autoencoder.input, autoencoder.get_layer('Latent_layer').output)\n",
    "    X_train_encode = encoder.predict(X_train)\n",
    "    X_test_encode = encoder.predict(X_test)\n",
    "\n",
    "    pd.DataFrame(X_train_encode).to_csv(\"../../1_Clasifications_models/data_reduced/DAE/X_train_Norm_DAE_\" + folders[index_data] + \".csv\", index=False)\n",
    "    pd.DataFrame(X_test_encode).to_csv(\"../../1_Clasifications_models/data_reduced/DAE/X_test_Norm_DAE_\" + folders[index_data] + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
